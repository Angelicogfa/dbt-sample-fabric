# Treinamento dbt com Microsoft Fabric

Este documento descreve, de forma **coerente, l√≥gica e replic√°vel**, o passo a passo para cria√ß√£o e execu√ß√£o de um projeto de **dbt** integrado ao **Microsoft Fabric**, servindo como base para treinamentos e desenvolvimento.

---

## üìã 1. Pr√©-requisitos

Antes de iniciar, garanta que os seguintes itens estejam dispon√≠veis:

- Sistema operacional: **Linux, macOS ou Windows**
- **Python 3.8 ou superior**
- Acesso a um workspace do **Microsoft Fabric**
- **Azure CLI** instalado e configurado (para autentica√ß√£o CLI)
- **Microsoft ODBC Driver 18** ou superior para SQL Server
- **Git** para versionamento

### Instala√ß√£o do Azure CLI

- Windows: [Download Azure CLI](https://learn.microsoft.com/pt-br/cli/azure/install-azure-cli-windows)
- Linux/macOS: 
  ```bash
  curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
  ```

### Instala√ß√£o do ODBC Driver

- Windows: [Download ODBC Driver 18](https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server)
- Linux: [Instru√ß√µes de instala√ß√£o](https://learn.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server)

---

## üöÄ 2. Setup do Ambiente

### 2.1 Cria√ß√£o do diret√≥rio do projeto

```bash
mkdir treinamento-dbt
cd treinamento-dbt
```

### 2.2 Cria√ß√£o e ativa√ß√£o do ambiente virtual

**Criar ambiente virtual:**
```bash
python -m venv .venv
```

**Ativar ambiente virtual:**

- **Linux / macOS:**
  ```bash
  source .venv/bin/activate
  ```

- **Windows (PowerShell):**
  ```powershell
  .venv\Scripts\Activate.ps1
  ```

- **Windows (CMD):**
  ```cmd
  .venv\Scripts\activate.bat
  ```

### 2.3 Instala√ß√£o das depend√™ncias

Crie um arquivo `requirements.txt` na raiz do projeto:

```txt
# =====================================================
# DBT Dependencies
# =====================================================
dbt-core==1.11.2
dbt-fabric==1.9.3
dbt-sqlserver==1.9.0
python-dotenv==1.2.1
```

**Instale as depend√™ncias:**

```bash
pip install -r requirements.txt
```

**Verificar instala√ß√£o:**
```bash
dbt --version
```

---

## üìÅ 3. Cria√ß√£o do Projeto dbt

### 3.1 Inicializar projeto dbt

```bash
dbt init treinamento_dbt
```

Durante a inicializa√ß√£o, o dbt far√° algumas perguntas:
- **Which database would you like to use?** Escolha o n√∫mero correspondente ao **fabric**
- As demais configura√ß√µes ser√£o feitas via `profiles.yml`

### 3.2 Estrutura do projeto criada

```text
treinamento-dbt/
‚îú‚îÄ‚îÄ .venv/
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .env.sample
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ run_dbt.ps1
‚îú‚îÄ‚îÄ run_dbt.sh
‚îî‚îÄ‚îÄ treinamento_dbt/
    ‚îú‚îÄ‚îÄ .gitignore
    ‚îú‚îÄ‚îÄ .user.yml
    ‚îú‚îÄ‚îÄ dbt_project.yml
    ‚îú‚îÄ‚îÄ profiles.yml
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ analyses/
    ‚îú‚îÄ‚îÄ logs/
    ‚îú‚îÄ‚îÄ macros/
    ‚îú‚îÄ‚îÄ models/
    ‚îÇ   ‚îú‚îÄ‚îÄ staging/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lakehouse/
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ _lakehouse_sources.yml
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ stg_lakehouse__taxi.sql
    ‚îÇ   ‚îú‚îÄ‚îÄ intermediate/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lakehouse/
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ int_dim_date.sql
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ int_dim_location.sql
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ int_dim_payment_type.sql
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ int_dim_rate_code.sql
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ int_dim_time.sql
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ int_dim_vendor.sql
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ int_fct_taxi_trip.sql
    ‚îÇ   ‚îî‚îÄ‚îÄ marts/
    ‚îÇ       ‚îú‚îÄ‚îÄ dimensions/
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dim_date.sql
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dim_date.yml
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dim_location.sql
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dim_location.yml
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dim_payment_type.sql
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dim_payment_type.yml
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dim_rate_code.sql
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dim_rate_code.yml
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dim_time.sql
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dim_time.yml
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dim_vendor.sql
    ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ dim_vendor.yml
    ‚îÇ       ‚îî‚îÄ‚îÄ facts/
    ‚îÇ           ‚îú‚îÄ‚îÄ fct_taxi_trip.sql
    ‚îÇ           ‚îî‚îÄ‚îÄ fct_taxi_trip.yml
    ‚îú‚îÄ‚îÄ seeds/
    ‚îú‚îÄ‚îÄ snapshots/
    ‚îú‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ target/
        ‚îú‚îÄ‚îÄ manifest.json
        ‚îú‚îÄ‚îÄ run_results.json
        ‚îú‚îÄ‚îÄ compiled/
        ‚îî‚îÄ‚îÄ run/
```

---

## üîê 4. Configura√ß√£o de Vari√°veis de Ambiente

### 4.1 Criar arquivo `.env`

Crie um arquivo `.env` na **raiz do projeto** (fora da pasta `treinamento_dbt`):

```env
# =====================================================
# DBT Database Configuration
# =====================================================

# ========== PERFIL ATIVO ==========
# Escolha: 'fabric_local' (desenvolvimento) ou 'fabric' (produ√ß√£o)
DBT_TARGET=fabric_local

# ========== CONFIGURA√á√ÉO FABRIC LOCAL (Desenvolvimento com az login) ==========
# Use este perfil para desenvolvimento com suas credenciais Azure
# Requisito: Execute 'az login' antes de usar o DBT
FABRIC_SERVER=seu-workspace.datawarehouse.fabric.microsoft.com
FABRIC_DATABASE=DataWarehouseTreinamento
FABRIC_SCHEMA=dbo

# ========== CONFIGURA√á√ÉO FABRIC (Service Principal - Produ√ß√£o/CI/CD) ==========
# Use este perfil para automa√ß√£o e pipelines
# Descomente e configure para usar Fabric em produ√ß√£o:
# FABRIC_TENANT_ID=your-tenant-id
# FABRIC_CLIENT_ID=your-client-id
# FABRIC_CLIENT_SECRET=your-client-secret

# ========== CONFIGURA√á√ïES GERAIS ==========
ODBC_DRIVER=ODBC Driver 18 for SQL Server
DBT_THREADS=4
```

> **‚ö†Ô∏è IMPORTANTE:** Nunca versione o arquivo `.env` com credenciais reais!

### 4.2 Criar arquivo `.env.example`

Crie um template sem credenciais para versionamento:

```env
# =====================================================
# DBT Database Configuration - TEMPLATE
# =====================================================
# Copie este arquivo para .env e preencha com suas credenciais

DBT_TARGET=fabric_local

# Fabric Configuration (Development)
FABRIC_SERVER=your-workspace.datawarehouse.fabric.microsoft.com
FABRIC_DATABASE=DataWarehouseTreinamento
FABRIC_SCHEMA=dbo

# Fabric Configuration (Production - Service Principal)
# FABRIC_TENANT_ID=your-tenant-id
# FABRIC_CLIENT_ID=your-client-id
# FABRIC_CLIENT_SECRET=your-client-secret

ODBC_DRIVER=ODBC Driver 18 for SQL Server
DBT_THREADS=4
```

### 4.3 Configurar `.gitignore`

Crie ou atualize o arquivo `.gitignore` na raiz:

```gitignore
# DBT
target/
dbt_packages/
logs/
dbt_modules/

# Python
__pycache__/
*.py[cod]
*$py.class
.Python
.venv/
venv/
ENV/
env/

# Ambiente e Credenciais
.env
.env.local
.env.*.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Outros
*.log
```

---

## ‚öôÔ∏è 5. Configura√ß√£o do Profiles do dbt

### 5.1 Criar `treinamento_dbt/profiles.yml`

Crie ou substitua o conte√∫do do arquivo `treinamento_dbt/profiles.yml`:

```yaml
treinamento_dbt:
  target: "{{ env_var('DBT_TARGET', 'fabric_local') }}"
  outputs:
    # Perfil para Microsoft Fabric Local (Desenvolvimento com az login)
    fabric_local:
      type: fabric
      driver: "{{ env_var('ODBC_DRIVER', 'ODBC Driver 18 for SQL Server') }}"
      server: "{{ env_var('FABRIC_SERVER') }}"
      port: 1433
      database: "{{ env_var('FABRIC_DATABASE', 'DataWarehouseTreinamento') }}"
      schema: "{{ env_var('FABRIC_SCHEMA', 'dbo') }}"
      threads: "{{ env_var('DBT_THREADS', '4') | int }}"
      authentication: CLI
      encrypt: true
      trust_cert: false
    
    # Perfil para Microsoft Fabric (Service Principal - Produ√ß√£o/CI/CD)
    fabric:
      type: fabric
      driver: "{{ env_var('ODBC_DRIVER', 'ODBC Driver 18 for SQL Server') }}"
      server: "{{ env_var('FABRIC_SERVER') }}"
      port: 1433
      database: "{{ env_var('FABRIC_DATABASE', 'DataWarehouseTreinamento') }}"
      schema: "{{ env_var('FABRIC_SCHEMA', 'dbo') }}"
      threads: "{{ env_var('DBT_THREADS', '4') | int }}"
      authentication: ServicePrincipal
      tenant_id: "{{ env_var('FABRIC_TENANT_ID') }}"
      client_id: "{{ env_var('FABRIC_CLIENT_ID') }}"
      client_secret: "{{ env_var('FABRIC_CLIENT_SECRET') }}"
      encrypt: true
      trust_cert: false
```

### 5.2 Entendendo os perfis

**‚òÅÔ∏è fabric_local** - Desenvolvimento com Microsoft Fabric
- Usa autentica√ß√£o CLI (`az login`)
- N√£o precisa de Service Principal
- Usa suas credenciais pessoais do Azure

**üè≠ fabric** - Produ√ß√£o/CI/CD com Microsoft Fabric
- Usa autentica√ß√£o Service Principal
- Ideal para automa√ß√£o e pipelines
- Requer configura√ß√£o de App Registration no Azure

---

## üõ†Ô∏è 6. Scripts de Execu√ß√£o

### 6.1 Script PowerShell (Windows)

Crie o arquivo `run_dbt.ps1` na raiz do projeto:

```powershell
# Script para carregar vari√°veis de ambiente do .env e executar comandos DBT
# Uso: .\run_dbt.ps1 debug
#      .\run_dbt.ps1 run
#      .\run_dbt.ps1 test

param(
    [Parameter(Mandatory=$false)]
    [string]$Command = "debug"
)

# Carrega as vari√°veis do arquivo .env
$envFile = Join-Path $PSScriptRoot ".env"

if (Test-Path $envFile) {
    Write-Host "Carregando vari√°veis de ambiente de $envFile..." -ForegroundColor Green
    
    Get-Content $envFile | ForEach-Object {
        if ($_ -match '^\s*([^#][^=]+)=(.*)$') {
            $key = $matches[1].Trim()
            $value = $matches[2].Trim()
            
            # Remove aspas se existirem
            $value = $value -replace '^["'']|["'']$', ''
            
            # Define a vari√°vel de ambiente
            [Environment]::SetEnvironmentVariable($key, $value, "Process")
            Write-Host "  $key = $value" -ForegroundColor Cyan
        }
    }
    Write-Host ""
} else {
    Write-Host "Arquivo .env n√£o encontrado em $envFile" -ForegroundColor Yellow
}

# Ativa o ambiente virtual se existir
$venvPath = Join-Path $PSScriptRoot ".venv\Scripts\Activate.ps1"
if (Test-Path $venvPath) {
    Write-Host "Ativando ambiente virtual..." -ForegroundColor Green
    & $venvPath
    Write-Host ""
}

# Navega para o diret√≥rio do DBT
Set-Location (Join-Path $PSScriptRoot "treinamento_dbt")

# Executa o comando DBT
Write-Host "Executando: dbt $Command" -ForegroundColor Green
Write-Host ""

$dbtArgs = $Command -split ' '
& dbt @dbtArgs

# Retorna ao diret√≥rio original
Set-Location $PSScriptRoot
```

### 6.2 Script Bash (Linux/macOS)

Crie o arquivo `run_dbt.sh` na raiz do projeto:

```bash
#!/bin/bash
# Script para carregar vari√°veis de ambiente do .env e executar comandos DBT
# Uso: ./run_dbt.sh debug
#      ./run_dbt.sh run
#      ./run_dbt.sh test

# Carrega as vari√°veis do arquivo .env
if [ -f .env ]; then
    echo "Carregando vari√°veis de ambiente de .env..."
    export $(grep -v '^#' .env | xargs)
    echo ""
else
    echo "Arquivo .env n√£o encontrado"
    exit 1
fi

# Ativa o ambiente virtual se existir
if [ -f .venv/bin/activate ]; then
    echo "Ativando ambiente virtual..."
    source .venv/bin/activate
    echo ""
fi

# Navega para o diret√≥rio do DBT
cd treinamento_dbt

# Executa o comando DBT
echo "Executando: dbt $@"
echo ""
dbt "$@"

# Retorna ao diret√≥rio original
cd ..
```

**Tornar execut√°vel (Linux/macOS):**
```bash
chmod +x run_dbt.sh
```

---

## ‚úÖ 7. Valida√ß√£o da Configura√ß√£o

### 7.1 Testar conex√£o

**Windows:**
```powershell
.\run_dbt.ps1 debug
```

**Linux/macOS:**
```bash
./run_dbt.sh debug
```

### 7.2 Interpretar resultados

‚úÖ **Sucesso** - Todas as verifica√ß√µes passaram:
```
Configuration:
  profiles.yml file [OK found and valid]
  dbt_project.yml file [OK found and valid]
Connection test: [OK connection ok]
```

‚ùå **Erro** - Revise:
- Vari√°veis de ambiente no `.env`
- Driver ODBC instalado corretamente
- Credenciais v√°lidas
- Servidor acess√≠vel
- Para `fabric_local`: Execute `az login` primeiro

---

## üéØ 8. Execu√ß√£o do dbt

### 8.1 Comandos principais

**Validar conex√£o:**
```powershell
.\run_dbt.ps1 debug
```

**Executar todos os modelos:**
```powershell
.\run_dbt.ps1 run
```

**Executar modelo espec√≠fico:**
```powershell
.\run_dbt.ps1 "run --select my_model"
```

**Executar testes:**
```powershell
.\run_dbt.ps1 test
```

**Gerar documenta√ß√£o:**
```powershell
.\run_dbt.ps1 "docs generate"
.\run_dbt.ps1 "docs serve"
```

**Compilar sem executar:**
```powershell
.\run_dbt.ps1 compile
```

### 8.2 Alternar entre perfis

**Op√ß√£o 1 - Editar `.env`:**
```env
DBT_TARGET=fabric_local  # Para desenvolvimento com az login (padr√£o)
DBT_TARGET=fabric        # Para produ√ß√£o com Service Principal
```

**Op√ß√£o 2 - Vari√°vel tempor√°ria (PowerShell):**
```powershell
$env:DBT_TARGET="fabric_local"; .\run_dbt.ps1 debug
```

**Op√ß√£o 3 - Vari√°vel tempor√°ria (Bash):**
```bash
DBT_TARGET=fabric_local ./run_dbt.sh debug
```

---

## üîë 9. Configura√ß√£o de Autentica√ß√£o Fabric

### 9.1 Autentica√ß√£o CLI (fabric_local)

**Passo 1 - Login no Azure:**
```bash
az login
```

**Passo 2 - Verificar conta:**
```bash
az account show
```

**Passo 3 - Configurar `.env`:**
```env
DBT_TARGET=fabric_local
FABRIC_SERVER=seu-workspace.datawarehouse.fabric.microsoft.com
FABRIC_DATABASE=DataWarehouseTreinamento
FABRIC_SCHEMA=dbo
```

### 9.2 Autentica√ß√£o Service Principal (fabric)

**Passo 1 - Criar App Registration no Azure:**
1. Portal Azure ‚Üí Azure Active Directory
2. App registrations ‚Üí New registration
3. Copiar: **Application (client) ID**
4. Copiar: **Directory (tenant) ID**

**Passo 2 - Criar Client Secret:**
1. Certificates & secrets ‚Üí New client secret
2. Copiar o **Value** (aparece s√≥ uma vez!)

**Passo 3 - Dar permiss√µes no Fabric:**
1. Fabric workspace ‚Üí Settings ‚Üí Manage access
2. Adicionar o Service Principal como Admin/Member

**Passo 4 - Configurar `.env`:**
```env
DBT_TARGET=fabric
FABRIC_SERVER=seu-workspace.datawarehouse.fabric.microsoft.com
FABRIC_DATABASE=DataWarehouseTreinamento
FABRIC_SCHEMA=dbo
FABRIC_TENANT_ID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
FABRIC_CLIENT_ID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
FABRIC_CLIENT_SECRET=seu_secret_value
```

---

## ÔøΩ 10. Entendendo a Arquitetura do Projeto

Antes de come√ßarmos a construir os modelos, √© importante entender a arquitetura que seguiremos.

### 10.1 Arquitetura Medallion em 3 Camadas

Este projeto segue a arquitetura **Medallion**, organizando os dados em **3 camadas l√≥gicas**:

```text
ü•â BRONZE (Staging)      ‚Üí  ü•à SILVER (Intermediate)  ‚Üí  ü•á GOLD (Marts)
   Ingest√£o Bruta             Transforma√ß√µes                Consumo Final
   Views                      Views Reutiliz√°veis           Tables/Incremental
```

| Camada | Prop√≥sito | Materializa√ß√£o | Nomenclatura |
|--------|-----------|----------------|--------------|
| **ü•â Staging** | Ingest√£o 1:1 com fontes, limpeza b√°sica | `view` | `stg_{sistema}__{entidade}` |
| **ü•à Intermediate** | Transforma√ß√µes reutiliz√°veis, regras de neg√≥cio | `view` | `int_{conceito}` |
| **ü•á Marts** | Modelos finais para consumo (dimens√µes/fatos) | `table`/`incremental` | `dim_{entidade}` ou `fct_{processo}` |

### 10.2 Configura√ß√£o no dbt_project.yml

O arquivo `dbt_project.yml` j√° est√° configurado com estas defini√ß√µes:

```yaml
models:
  treinamento_dbt:
    staging:
      +materialized: view
      +schema: staging
    intermediate:
      +materialized: view
      +schema: intermediate
    marts:
      +materialized: table
      +schema: marts
```

### 10.3 Resultado no Microsoft Fabric

Quando executamos `dbt run`, o dbt criar√° os seguintes schemas no Fabric:

```text
DataWarehouseTreinamento/
‚îú‚îÄ‚îÄ dbo_staging/          # Views de ingest√£o
‚îú‚îÄ‚îÄ dbo_intermediate/     # Views de transforma√ß√£o
‚îî‚îÄ‚îÄ dbo_marts/            # Tabelas finais (dimensions + facts)
```

---

## üèóÔ∏è 11. CONSTRUINDO: Camada Staging (Bronze)

Agora vamos construir nossa primeira camada! A camada de **staging** faz a ingest√£o dos dados brutos.

### 11.1 Passo 1: Criar o diret√≥rio de staging

```bash
mkdir -p treinamento_dbt/models/staging/lakehouse
```

### 11.2 Passo 2: Documentar a Source

Crie o arquivo `treinamento_dbt/models/staging/lakehouse/_lakehouse_sources.yml`:

```yaml
version: 2

sources:
  - name: lakehouse_treinamento
    description: "Lakehouse contendo dados para treinamento do time"
    database: LakehouseTreinamento
    schema: dbo
    
    # Alertas se dados estiverem desatualizados
    freshness:
      warn_after: {count: 24, period: hour}
      error_after: {count: 7, period: day}
    
    tables:
      - name: taxi
        loaded_at_field: lpepPickupDatetime
        description: "Dados de viagens de t√°xi do lakehouse"
        columns:
          - name: vendorID
            description: "ID do fornecedor de dados (1=CMT, 2=VTS)"
            tests:
              - not_null
          
          - name: lpepPickupDatetime
            description: "Data e hora de in√≠cio da viagem"
            tests:
              - not_null
          
          - name: lpepDropoffDatetime
            description: "Data e hora de t√©rmino da viagem"
          
          - name: passengerCount
            description: "N√∫mero de passageiros na viagem"
          
          - name: tripDistance
            description: "Dist√¢ncia da viagem em milhas"
          
          - name: puLocationId
            description: "ID da localiza√ß√£o de pickup"
          
          - name: doLocationId
            description: "ID da localiza√ß√£o de dropoff"
          
          - name: paymentType
            description: "Tipo de pagamento (1=Cart√£o, 2=Dinheiro, 3=Sem cobran√ßa, etc.)"
          
          - name: fareAmount
            description: "Valor da tarifa"
          
          - name: totalAmount
            description: "Valor total da viagem"
```

### 11.3 Passo 3: Testar a Source

Verifique se o dbt consegue ler a source:

```powershell
.\run_dbt.ps1 "source freshness"
```

‚úÖ **Resultado esperado:** Confirma√ß√£o de que a tabela existe e est√° acess√≠vel.

### 11.4 Passo 4: Criar o Modelo Staging

Crie o arquivo `treinamento_dbt/models/staging/lakehouse/stg_lakehouse__taxi.sql`:

```sql

with source as (
    -- L√™ os dados da source documentada
    select * 
    from {{ source('lakehouse_treinamento', 'taxi') }}
),

filtered_source as (
    -- Filtra apenas dados at√© 2019
    select * 
    from source
    where year(lpepPickupDatetime) <= 2019 
       or year(lpepDropoffDatetime) <= 2019
),

unique_row as (
    -- Remove duplicatas mantendo maior totalAmount
    select *,
        ROW_NUMBER() OVER (
            PARTITION BY vendorID, lpepPickupDatetime 
            ORDER BY totalAmount DESC
        ) AS row_num
    from filtered_source
),

filtered as (
    -- Mant√©m apenas primeira linha de cada grupo
    select * 
    from unique_row
    where row_num = 1
),

with_sk_id as (
    -- Cria surrogate key √∫nica para cada viagem
    select *,
        HASHBYTES(
            'SHA2_256', 
            CONCAT_WS(
                '|',
                vendorID,
                CAST(lpepPickupDatetime AS VARCHAR(50))
            )
        ) AS sk_id
    from filtered
)

-- Retorna todos os campos incluindo a surrogate key
select * 
from with_sk_id
```

### 11.5 Passo 5: Executar o Modelo Staging

```powershell
.\run_dbt.ps1 "run --select stg_lakehouse__taxi"
```

‚úÖ **Resultado esperado:**
```
Completed successfully
1 of 1 OK created view model dbo_staging.stg_lakehouse__taxi
```

### 11.6 Passo 6: Explorar os Dados Criados

Agora que a view foi criada, vamos explorar os dados para entender o que foi constru√≠do.

**Consulta 1: Ver estrutura e primeiros registros**
```sql
-- Ver as primeiras 10 viagens
SELECT TOP 10 
    vendorID,
    lpepPickupDatetime,
    lpepDropoffDatetime,
    passengerCount,
    tripDistance,
    fareAmount,
    totalAmount,
    sk_id  -- Surrogate key criada pelo staging
FROM dbo_staging.stg_lakehouse__taxi
ORDER BY lpepPickupDatetime DESC;
```

**Consulta 2: Validar remo√ß√£o de duplicatas**
```sql
-- Verificar se existem duplicatas (deve retornar 0)
SELECT 
    vendorID,
    lpepPickupDatetime,
    COUNT(*) as qtd_registros
FROM dbo_staging.stg_lakehouse__taxi
GROUP BY vendorID, lpepPickupDatetime
HAVING COUNT(*) > 1;
```

**Consulta 3: Estat√≠sticas gerais**
```sql
-- Estat√≠sticas dos dados de staging
SELECT 
    COUNT(*) as total_viagens,
    COUNT(DISTINCT vendorID) as total_vendors,
    MIN(lpepPickupDatetime) as primeira_viagem,
    MAX(lpepPickupDatetime) as ultima_viagem,
    AVG(tripDistance) as distancia_media,
    AVG(totalAmount) as valor_medio,
    SUM(totalAmount) as receita_total
FROM dbo_staging.stg_lakehouse__taxi;
```

**Consulta 4: Distribui√ß√£o por vendor**
```sql
-- Quantas viagens por fornecedor
SELECT 
    vendorID,
    COUNT(*) as total_viagens,
    AVG(tripDistance) as distancia_media,
    AVG(totalAmount) as valor_medio,
    MIN(lpepPickupDatetime) as primeira_viagem,
    MAX(lpepPickupDatetime) as ultima_viagem
FROM dbo_staging.stg_lakehouse__taxi
GROUP BY vendorID
ORDER BY vendorID;
```

**Consulta 5: Validar filtro de ano**
```sql
-- Verificar se todas as viagens s√£o at√© 2019
SELECT 
    YEAR(lpepPickupDatetime) as ano_pickup,
    YEAR(lpepDropoffDatetime) as ano_dropoff,
    COUNT(*) as quantidade
FROM dbo_staging.stg_lakehouse__taxi
GROUP BY YEAR(lpepPickupDatetime), YEAR(lpepDropoffDatetime)
ORDER BY ano_pickup, ano_dropoff;
```

üí° **Interpreta√ß√£o dos Resultados:**
- **Total de viagens**: Deve mostrar todas as viagens ap√≥s remo√ß√£o de duplicatas
- **Surrogate key (sk_id)**: Cada viagem tem um identificador √∫nico em hash
- **Anos**: Apenas viagens at√© 2019 devem aparecer
- **Vendors**: Normalmente 2 vendors (1=CMT, 2=VTS)

### 11.7 Passo 7: Validar Qualidade dos Dados

Execute consultas para garantir a qualidade:

```sql
-- 1. Verificar registros com valores nulos em campos cr√≠ticos
SELECT 
    COUNT(*) as total,
    COUNT(vendorID) as com_vendor,
    COUNT(lpepPickupDatetime) as com_pickup_date,
    COUNT(totalAmount) as com_total_amount,
    COUNT(*) - COUNT(vendorID) as sem_vendor,
    COUNT(*) - COUNT(lpepPickupDatetime) as sem_pickup_date
FROM dbo_staging.stg_lakehouse__taxi;

-- 2. Verificar viagens com valores negativos (n√£o deveria haver)
SELECT 
    COUNT(*) as viagens_valor_negativo
FROM dbo_staging.stg_lakehouse__taxi
WHERE totalAmount < 0 OR fareAmount < 0 OR tripDistance < 0;

-- 3. Verificar viagens com data de dropoff antes de pickup (anomalia)
SELECT 
    COUNT(*) as viagens_anomalas,
    MIN(DATEDIFF(MINUTE, lpepPickupDatetime, lpepDropoffDatetime)) as menor_duracao_minutos
FROM dbo_staging.stg_lakehouse__taxi
WHERE lpepDropoffDatetime < lpepPickupDatetime;
```

‚úÖ **Resultados esperados:**
- Campos cr√≠ticos n√£o devem ter nulos (vendorID, lpepPickupDatetime)
- N√£o deve haver valores negativos em dist√¢ncia/valores
- Dropoff sempre deve ser ap√≥s pickup

### 11.8 Passo 8: Validar no Fabric (Interface Gr√°fica)

Acesse o **Microsoft Fabric** e verifique:
1. Schema `dbo_staging` foi criado
2. View `stg_lakehouse__taxi` existe
3. Consulte alguns registros para validar

```sql
SELECT TOP 10 * 
FROM dbo_staging.stg_lakehouse__taxi
```

‚úÖ **Checkpoint:** Camada Staging criada com sucesso! Voc√™ agora tem uma view limpa dos dados brutos com qualidade validada.

---

## üîÑ 12. CONSTRUINDO: Camada Intermediate (Silver)

A camada intermediate prepara componentes reutiliz√°veis. Aqui criaremos **6 dimens√µes intermedi√°rias** a partir dos dados de t√°xi.

### 12.1 Criar o diret√≥rio intermediate

```bash
mkdir -p treinamento_dbt/models/intermediate/lakehouse
```

### 12.2 Modelo 1: int_dim_date (Calend√°rio Completo)

**Objetivo:** Gerar todas as datas entre a menor e maior data das viagens com atributos de calend√°rio.

Crie `treinamento_dbt/models/intermediate/lakehouse/int_dim_date.sql`:

```sql
WITH source_data AS (
    SELECT *
    from {{ ref('stg_lakehouse__taxi') }}
    WHERE year(lpepPickupDatetime) <= 2019 OR year(lpepDropoffDatetime) <= 2019
),
date_bounds AS (
    SELECT 
        MIN(CAST(lpepPickupDatetime AS DATE)) AS min_date,
        MAX(CAST(lpepPickupDatetime AS DATE)) AS max_date
    FROM source_data
),
-- Number generator (0 to 9)
ten_rows AS (
    SELECT 1 AS n UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL
    SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1
),
-- Multiplier to generate a large sequence of days
number_series AS (
    SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS n
    FROM ten_rows a
    CROSS JOIN ten_rows b
    CROSS JOIN ten_rows c
    CROSS JOIN ten_rows d
    CROSS JOIN ten_rows e
),
-- Intermediate CTE: Generates raw dates first
raw_dates AS (
    SELECT 
        CAST(DATEADD(DAY, ns.n, db.min_date) AS DATE) AS date_value
    FROM date_bounds db
    CROSS JOIN number_series ns
    WHERE ns.n <= DATEDIFF(DAY, db.min_date, db.max_date)
)
-- FINAL SELECT: Calculates all time dimensions
SELECT 
    date_value AS date,
    
    -- Basic Information
    YEAR(date_value) AS year,
    MONTH(date_value) AS month,
    DAY(date_value) AS day_of_month,
    
    -- Text Formatting (Depends on server language settings, e.g., 'January')
    CAST(DATENAME(MONTH, date_value) AS VARCHAR(20)) AS month_name,
    CAST(LEFT(DATENAME(MONTH, date_value), 3) AS VARCHAR(3)) AS month_name_abbrev,
    CAST(DATENAME(WEEKDAY, date_value) AS VARCHAR(20)) AS day_of_week_name,
    CAST(LEFT(DATENAME(WEEKDAY, date_value), 3) AS VARCHAR(3)) AS day_of_week_abbrev,
    
    -- Day of Week Number (1 to 7). Note: Depends on SET DATEFIRST
    DATEPART(WEEKDAY, date_value) AS day_of_week_num,
    
    -- Weekend Indicator
    -- Note: Ensure your session is in English or adjust 'Saturday'/'Sunday' accordingly
    CASE 
        WHEN DATENAME(WEEKDAY, date_value) IN ('Saturday', 'Sunday') THEN 1 
        ELSE 0 
    END AS is_weekend,

    -- Quarter
    DATEPART(QUARTER, date_value) AS quarter,
    CAST(YEAR(date_value) AS VARCHAR(4)) + '-Q' + CAST(DATEPART(QUARTER, date_value) AS VARCHAR(1)) AS year_quarter, -- Ex: 2019-Q1

    -- Bimester (Math: (Month-1)/2 + 1)
    ((MONTH(date_value) - 1) / 2) + 1 AS bimester,
    
    -- Semester
    CASE WHEN MONTH(date_value) <= 6 THEN 1 ELSE 2 END AS semester,
    CAST(YEAR(date_value) AS VARCHAR(4)) + '-S' + CAST(CASE WHEN MONTH(date_value) <= 6 THEN 1 ELSE 2 END AS VARCHAR(1)) AS year_semester, -- Ex: 2019-S1

    -- Fortnight (Rule: Day <= 15 is the 1st fortnight)
    CASE WHEN DAY(date_value) <= 15 THEN 1 ELSE 2 END AS fortnight,
    
    -- Day of Year (1 to 365/366)
    DATEPART(DAYOFYEAR, date_value) AS day_of_year,
    
    -- Week of Year (1 to 53)
    DATEPART(WEEK, date_value) AS week_of_year

FROM raw_dates
```

### 12.3 Modelo 2: int_dim_location (Localiza√ß√µes)

**Objetivo:** Extrair todos os IDs √∫nicos de localiza√ß√£o (pickup + dropoff).

Crie `treinamento_dbt/models/intermediate/lakehouse/int_dim_location.sql`:

```sql
with sg_taxi as (
    select *
    from {{ ref('stg_lakehouse__taxi') }}
),
location as (
    select
        distinct puLocationId as location_id
    from sg_taxi
    union
    select
        distinct doLocationId as location_id
    from sg_taxi
),
dim_location as (
    select
        distinct location_id,
        HASHBYTES(
            'SHA2_256',
            CAST(location_id AS VARCHAR(50))
        ) AS sk_location_id
    from location
)
select *
from dim_location
```

### 12.4 Modelo 3: int_dim_vendor (Fornecedores)

**Objetivo:** Criar dimens√£o de fornecedores com nomes descritivos.

Crie `treinamento_dbt/models/intermediate/lakehouse/int_dim_vendor.sql`:

```sql
WITH vendor_data AS (
    -- Buscamos os IDs √∫nicos da camada de staging
    SELECT DISTINCT 
        CAST("vendorID" AS INT) AS vendor_id
    from {{ ref('stg_lakehouse__taxi') }}
    WHERE "vendorID" IS NOT NULL
)
SELECT
    vendor_id,
    CASE vendor_id
        WHEN 1 THEN 'Creative Mobile Technologies'
        WHEN 2 THEN 'VeriFone Inc.'
        ELSE 'Unknown/Other'
    END AS vendor_name,
    CASE vendor_id
        WHEN 1 THEN 'CMT'
        WHEN 2 THEN 'VTS'
        ELSE 'UNK'
    END AS vendor_abbreviation
FROM vendor_data
```

### 12.5 Modelo 4: int_dim_payment_type (Tipos de Pagamento)

Crie `treinamento_dbt/models/intermediate/lakehouse/int_dim_payment_type.sql`:

```sql
WITH data AS (
    SELECT DISTINCT paymentType as payment_type
    FROM {{ ref('stg_lakehouse__taxi') }}
    WHERE paymentType IS NOT NULL
)

SELECT
    payment_type,
    CASE payment_type
        WHEN 1 THEN 'Credit card'
        WHEN 2 THEN 'Cash'
        WHEN 3 THEN 'No charge'
        WHEN 4 THEN 'Dispute'
        WHEN 5 THEN 'Unknown'
        ELSE 'Not Specified'
    END AS payment_type_name
FROM data
```

### 12.6 Modelo 5: int_dim_rate_code (C√≥digos de Tarifa)

Crie `treinamento_dbt/models/intermediate/lakehouse/int_dim_rate_code.sql`:

```sql
WITH unique_codes AS (
    SELECT DISTINCT rateCodeID as rate_code_id    
    from {{ ref('stg_lakehouse__taxi') }}
    WHERE rateCodeID IS NOT NULL
)
SELECT
    rate_code_id,
    CASE rate_code_id
        WHEN 1 THEN 'Standard rate'
        WHEN 2 THEN 'JFK'
        WHEN 3 THEN 'Newark'
        WHEN 4 THEN 'Nassau or Westchester'
        WHEN 5 THEN 'Negotiated fare'
        WHEN 6 THEN 'Group ride'
        WHEN 99 THEN 'Special/Unknown'
        ELSE 'Not Specified'
    END AS rate_code_name,
    CASE 
        WHEN rate_code_id IN (2, 3) THEN 1 
        ELSE 0 
    END AS is_airport_trip
FROM unique_codes
```

### 12.7 Modelo 6: int_dim_time (Hor√°rios do Dia)

Crie `treinamento_dbt/models/intermediate/lakehouse/int_dim_time.sql`:

```sql
WITH ten_rows AS (
    SELECT 1 AS n UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL
    SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1
),
-- Gerar 1440 n√∫meros (0 a 1439 minutos em um dia)
minute_series AS (
    SELECT TOP (1440) ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS minute_offset
    FROM ten_rows a CROSS JOIN ten_rows b CROSS JOIN ten_rows c CROSS JOIN ten_rows d
),
raw_time AS (
    SELECT 
        DATEADD(MINUTE, minute_offset, CAST('00:00:00' AS TIME)) AS time_value
    FROM minute_series
)
SELECT 
    CAST(time_value AS TIME(0)) AS time, -- Chave Prim√°ria (precis√£o 0 = sem fra√ß√µes de segundo)
    DATEPART(HOUR, time_value) AS hour,
    DATEPART(MINUTE, time_value) AS minute,
    
    -- Formata√ß√£o amig√°vel
    CAST(FORMAT(CAST(time_value AS DATETIME), 'HH:mm') AS VARCHAR(5)) AS time_24h,
    
    -- Per√≠odos do Dia (Essencial para an√°lise de mobilidade)
    CASE 
        WHEN DATEPART(HOUR, time_value) BETWEEN 0 AND 5 THEN 'Madrugada'
        WHEN DATEPART(HOUR, time_value) BETWEEN 6 AND 11 THEN 'Manh√£'
        WHEN DATEPART(HOUR, time_value) BETWEEN 12 AND 17 THEN 'Tarde'
        ELSE 'Noite'
    END AS period_of_day,

    -- Flags de Hor√°rio de Pico (Rush Hour - Adaptar conforme regra de NY)
    CASE 
        WHEN (DATEPART(HOUR, time_value) BETWEEN 7 AND 9) OR (DATEPART(HOUR, time_value) BETWEEN 16 AND 19) 
        THEN 1 ELSE 0 
    END AS is_rush_hour

FROM raw_time
```

### 12.8 Executar Todos os Modelos Intermediate

```powershell
.\run_dbt.ps1 "run --select intermediate"
```

‚úÖ **Resultado esperado:**
```
Completed successfully
6 of 6 OK created view model dbo_intermediate.int_dim_date
6 of 6 OK created view model dbo_intermediate.int_dim_location  
6 of 6 OK created view model dbo_intermediate.int_dim_vendor
6 of 6 OK created view model dbo_intermediate.int_dim_payment_type
6 of 6 OK created view model dbo_intermediate.int_dim_rate_code
6 of 6 OK created view model dbo_intermediate.int_dim_time
```

### 12.9 Explorar os Dados Criados

Agora vamos validar cada dimens√£o intermedi√°ria criada com consultas pr√°ticas.

**üìÖ Consulta 1: Dimens√£o de Data (int_dim_date)**
```sql
-- Ver primeiras e √∫ltimas datas do calend√°rio
SELECT TOP 5 
    date,
    year,
    month,
    month_name,
    day_of_week_name,
    is_weekend,
    quarter,
    year_quarter
FROM dbo_intermediate.int_dim_date
ORDER BY date;

-- √öltima data
SELECT TOP 5 
    date,
    day_of_week_name,
    is_weekend
FROM dbo_intermediate.int_dim_date
ORDER BY date DESC;

-- Estat√≠sticas do calend√°rio
SELECT 
    COUNT(*) as total_dias,
    MIN(date) as primeira_data,
    MAX(date) as ultima_data,
    COUNT(CASE WHEN is_weekend = 1 THEN 1 END) as dias_fim_semana,
    COUNT(CASE WHEN is_weekend = 0 THEN 1 END) as dias_uteis
FROM dbo_intermediate.int_dim_date;
```

**üìç Consulta 2: Dimens√£o de Localiza√ß√£o (int_dim_location)**
```sql
-- Ver todas as localiza√ß√µes √∫nicas
SELECT TOP 10
    location_id,
    sk_location_id
FROM dbo_intermediate.int_dim_location
ORDER BY location_id;

-- Estat√≠sticas
SELECT 
    COUNT(*) as total_localizacoes,
    MIN(location_id) as menor_id,
    MAX(location_id) as maior_id
FROM dbo_intermediate.int_dim_location;
```

**üöñ Consulta 3: Dimens√£o de Vendor (int_dim_vendor)**
```sql
-- Ver todos os vendors
SELECT 
    vendor_id,
    vendor_name,
    vendor_abbreviation
FROM dbo_intermediate.int_dim_vendor
ORDER BY vendor_id;
```

**üí≥ Consulta 4: Dimens√£o de Tipo de Pagamento (int_dim_payment_type)**
```sql
-- Ver todos os tipos de pagamento
SELECT 
    payment_type,
    payment_type_name
FROM dbo_intermediate.int_dim_payment_type
ORDER BY payment_type;
```

**üí∞ Consulta 5: Dimens√£o de Rate Code (int_dim_rate_code)**
```sql
-- Ver todos os c√≥digos de tarifa
SELECT 
    rate_code_id,
    rate_code_name
FROM dbo_intermediate.int_dim_rate_code
ORDER BY rate_code_id;
```

**üïê Consulta 6: Dimens√£o de Tempo (int_dim_time)**
```sql
-- Ver primeiros hor√°rios do dia
SELECT TOP 20
    time,
    hour,
    minute,
    period_of_day,
    is_rush_hour
FROM dbo_intermediate.int_dim_time
ORDER BY time;

-- Hor√°rios de pico (rush hour)
SELECT 
    time,
    hour,
    period_of_day
FROM dbo_intermediate.int_dim_time
WHERE is_rush_hour = 1
ORDER BY time;

-- Estat√≠sticas por per√≠odo do dia
SELECT 
    period_of_day,
    COUNT(*) as total_minutos,
    COUNT(CASE WHEN is_rush_hour = 1 THEN 1 END) as minutos_pico
FROM dbo_intermediate.int_dim_time
GROUP BY period_of_day
ORDER BY 
    CASE period_of_day
        WHEN 'Madrugada' THEN 1
        WHEN 'Manh√£' THEN 2
        WHEN 'Tarde' THEN 3
        WHEN 'Noite' THEN 4
    END;
```

**üìä Consulta 7: Resumo Geral de Todas as Dimens√µes**
```sql
-- Contagem de registros em cada dimens√£o intermediate
SELECT 'int_dim_date' as dimensao, COUNT(*) as total_registros 
FROM dbo_intermediate.int_dim_date
UNION ALL
SELECT 'int_dim_location', COUNT(*) 
FROM dbo_intermediate.int_dim_location
UNION ALL
SELECT 'int_dim_vendor', COUNT(*) 
FROM dbo_intermediate.int_dim_vendor
UNION ALL
SELECT 'int_dim_payment_type', COUNT(*) 
FROM dbo_intermediate.int_dim_payment_type
UNION ALL
SELECT 'int_dim_rate_code', COUNT(*) 
FROM dbo_intermediate.int_dim_rate_code
UNION ALL
SELECT 'int_dim_time', COUNT(*) 
FROM dbo_intermediate.int_dim_time
ORDER BY total_registros DESC;
```

üí° **Resultados Esperados:**
- **int_dim_date**: ~3.891 datas (do m√≠nimo ao m√°ximo das viagens)
- **int_dim_location**: ~264 localiza√ß√µes √∫nicas
- **int_dim_vendor**: 2 vendors (CMT e VTS)
- **int_dim_payment_type**: ~5 tipos de pagamento
- **int_dim_rate_code**: ~7 c√≥digos de tarifa
- **int_dim_time**: 1.440 registros (24h √ó 60min)

### 12.10 Validar no Fabric
```sql
-- Verificar quantidade de registros em cada view
SELECT COUNT(*) FROM dbo_intermediate.int_dim_date;        -- ~3.891 datas
SELECT COUNT(*) FROM dbo_intermediate.int_dim_location;    -- ~264 localiza√ß√µes
SELECT COUNT(*) FROM dbo_intermediate.int_dim_vendor;      -- 2 vendors
SELECT COUNT(*) FROM dbo_intermediate.int_dim_payment_type; -- ~5 tipos
SELECT COUNT(*) FROM dbo_intermediate.int_dim_rate_code;   -- ~7 c√≥digos
SELECT COUNT(*) FROM dbo_intermediate.int_dim_time;        -- 1.440 minutos (24h x 60min)
```

### 12.11 Criar Tabela Fato - int_fct_taxi_trip

Agora vamos criar a **tabela fato intermedi√°ria** que faz os JOINs entre a staging e todas as dimens√µes, aplicando as **surrogate keys** corretas.

**üîç Conceitos importantes:**

1. **Surrogate Keys**: Usamos `sk_location_id` (hash BINARY) para location ao inv√©s da natural key
2. **Separa√ß√£o Date/Time**: Criamos 4 FKs separadas (pickup_date, pickup_time, dropoff_date, dropoff_time) para an√°lise granular
3. **Truncamento de Tempo**: Truncamos segundos usando `DATEADD(MINUTE, DATEDIFF(MINUTE, 0, datetime), 0)` para garantir correla√ß√£o com dim_time (que tem granularidade de minutos)
4. **Filtros de Qualidade**: Removemos registros sem data/hora v√°lida ou sem dimens√µes obrigat√≥rias

**üìÑ Arquivo:** `treinamento_dbt/models/intermediate/lakehouse/int_fct_taxi_trip.sql`

```sql

WITH stg_taxi AS (
    SELECT *
    FROM {{ ref('stg_lakehouse__taxi') }}
    WHERE lpepPickupDatetime IS NOT NULL
      AND lpepDropoffDatetime IS NOT NULL
      AND lpepDropoffDatetime > lpepPickupDatetime
),

-- Dimens√µes intermediate para JOINs
int_location AS (
    SELECT *
    FROM {{ ref('int_dim_location') }}
),

int_vendor AS (
    SELECT *
    FROM {{ ref('int_dim_vendor') }}
),

int_payment_type AS (
    SELECT *
    FROM {{ ref('int_dim_payment_type') }}
),

int_rate_code AS (
    SELECT *
    FROM {{ ref('int_dim_rate_code') }}
),

-- JOINs com as dimens√µes aplicando surrogate keys
fact_base AS (
    SELECT
        -- Chave Prim√°ria da Fato (Surrogate Key da Staging)
        taxi.sk_id AS trip_id,
        
        -- Foreign Keys para Dimens√µes
        
        -- Dimens√£o de Data (Pickup e Dropoff) - usa DATE diretamente
        CAST(taxi.lpepPickupDatetime AS DATE) AS pickup_date_fk,
        CAST(taxi.lpepDropoffDatetime AS DATE) AS dropoff_date_fk,
        
        -- Dimens√£o de Tempo (Pickup e Dropoff) - TRUNCA para minutos inteiros
        CAST(
            DATEADD(
                MINUTE,
                DATEDIFF(MINUTE, 0, taxi.lpepPickupDatetime),
                0
            ) AS TIME(0)
        ) AS pickup_time_fk,
        CAST(
            DATEADD(
                MINUTE,
                DATEDIFF(MINUTE, 0, taxi.lpepDropoffDatetime),
                0
            ) AS TIME(0)
        ) AS dropoff_time_fk,
        
        -- Dimens√£o de Localiza√ß√£o (Pickup e Dropoff) - USA SURROGATE KEY
        loc_pickup.sk_location_id AS pickup_location_fk,
        loc_dropoff.sk_location_id AS dropoff_location_fk,
        
        -- Dimens√£o de Fornecedor - usa natural key
        vendor.vendor_id AS vendor_fk,
        
        -- Dimens√£o de Tipo de Pagamento - usa natural key
        payment.payment_type AS payment_type_fk,
        
        -- Dimens√£o de Rate Code - usa natural key
        rate.rate_code_id AS rate_code_fk,
        
        -- M√©tricas da Viagem (Fatos/Medidas)
        taxi.fareAmount,
        taxi.extra,
        taxi.mtaTax,
        taxi.improvementSurcharge,
        taxi.tipAmount,
        taxi.tollsAmount,
        taxi.totalAmount,
        taxi.tripDistance,
        
        -- Atributos Descritivos (Degenerates)
        taxi.passengerCount,
        taxi.tripType,
        taxi.storeAndFwdFlag,
        
        -- C√°lculo de dura√ß√£o da viagem em minutos
        DATEDIFF(MINUTE, taxi.lpepPickupDatetime, taxi.lpepDropoffDatetime) AS trip_duration_minutes
        
    FROM stg_taxi taxi
    
    -- JOIN com Localiza√ß√£o Pickup (usando surrogate key)
    LEFT JOIN int_location loc_pickup
        ON taxi.puLocationId = loc_pickup.location_id
    
    -- JOIN com Localiza√ß√£o Dropoff (usando surrogate key)
    LEFT JOIN int_location loc_dropoff
        ON taxi.doLocationId = loc_dropoff.location_id
    
    -- JOIN com Vendor
    LEFT JOIN int_vendor vendor
        ON CAST(taxi."vendorID" AS INT) = vendor.vendor_id
    
    -- JOIN com Payment Type
    LEFT JOIN int_payment_type payment
        ON taxi.paymentType = payment.payment_type
    
    -- JOIN com Rate Code
    LEFT JOIN int_rate_code rate
        ON taxi.rateCodeID = rate.rate_code_id
)

SELECT
    -- Chave Prim√°ria
    trip_id,
    
    -- Foreign Keys
    pickup_date_fk,
    dropoff_date_fk,
    pickup_time_fk,
    dropoff_time_fk,
    pickup_location_fk,
    dropoff_location_fk,
    vendor_fk,
    payment_type_fk,
    rate_code_fk,
    
    -- M√©tricas
    fareAmount,
    extra,
    mtaTax,
    improvementSurcharge,
    tipAmount,
    tollsAmount,
    totalAmount,
    tripDistance,
    trip_duration_minutes,
    
    -- Atributos Descritivos
    passengerCount,
    tripType,
    storeAndFwdFlag
    
FROM fact_base
WHERE 
    -- Garantir que conseguimos fazer JOIN com as dimens√µes principais
    pickup_location_fk IS NOT NULL 
    AND dropoff_location_fk IS NOT NULL
    AND vendor_fk IS NOT NULL
```

**üéØ Executar no dbt:**
```powershell
.\run_dbt.ps1 "run --select int_fct_taxi_trip"
```

**üîç Validar no Fabric:**
```sql
-- Verificar quantidade de registros na fato intermediate
SELECT COUNT(*) FROM dbo_intermediate.int_fct_taxi_trip;  -- ~26M+ registros

-- Validar JOINs com dimens√µes (amostra)
SELECT TOP 100
    f.trip_id,
    f.pickup_date_fk,
    f.pickup_time_fk,
    f.pickup_location_fk,
    f.vendor_fk,
    f.totalAmount,
    f.tripDistance,
    f.trip_duration_minutes
FROM dbo_intermediate.int_fct_taxi_trip f
ORDER BY f.lpepPickupDatetime DESC;

-- Validar correla√ß√£o com dim_time (deve encontrar match)
SELECT TOP 10
    f.pickup_time_fk,
    t.time,
    t.time_24h,
    t.period_of_day
FROM dbo_intermediate.int_fct_taxi_trip f
INNER JOIN dbo_marts.dim_time t ON f.pickup_time_fk = t.time
ORDER BY f.lpepPickupDatetime DESC;

-- Validar correla√ß√£o com dim_location usando surrogate key
SELECT TOP 10
    f.pickup_location_fk,
    l.sk_location_id,
    l.location_id
FROM dbo_intermediate.int_fct_taxi_trip f
INNER JOIN dbo_marts.dim_location l ON f.pickup_location_fk = l.sk_location_id
WHERE f.pickup_location_fk IS NOT NULL;
```

üí° **Ponto de Aten√ß√£o:**
- O truncamento de tempo com `DATEADD(MINUTE, DATEDIFF(MINUTE, 0, datetime), 0)` √© **essencial** para garantir que o JOIN com dim_time funcione corretamente
- dim_time tem granularidade de **minutos** (1.440 registros), sem segundos
- Se n√£o truncar, TIME(0) mant√©m os segundos (ex: 14:35:42) e n√£o encontra match com dim_time (14:35:00)

‚úÖ **Checkpoint:** Camada Intermediate criada! Agora temos componentes reutiliz√°veis prontos e validados com dados reais.

---

## üéØ 13. CONSTRUINDO: Camada Marts - Dimensions (Gold)

Agora vamos criar as **dimens√µes finais** do Data Warehouse. Estas ser√£o **tabelas materializadas** para melhor performance.

### 13.1 Criar o diret√≥rio marts

```bash
mkdir -p treinamento_dbt/models/marts/dimensions
```

### 13.2 Dimens√£o 1: dim_date (Incremental)

**Objetivo:** Dimens√£o de data otimizada com carga incremental.

Crie `treinamento_dbt/models/marts/dimensions/dim_date.sql`:

```sql
-- =====================================================
-- Marts: Dimens√£o de Data (INCREMENTAL)
-- =====================================================
-- Dimens√£o conformed de data para todo o Data Warehouse
-- Materializa√ß√£o: INCREMENTAL para performance
-- =====================================================

{{
    config(
        materialized='incremental',
        unique_key='date',
        on_schema_change='fail'
    )
}}

WITH data AS (
    SELECT *
    FROM {{ ref ('int_dim_date') }}
    {% if is_incremental() %}
    -- Processa apenas datas que ainda n√£o existem na tabela
    WHERE date > (SELECT MAX(date) FROM {{ this }})
    {% endif %}
)

SELECT 
    date,
    year,
    month,
    day_of_month,
    month_name,
    month_name_abbrev,
    day_of_week_name,
    day_of_week_abbrev,
    day_of_week_num,
    is_weekend,
    quarter,
    year_quarter,
    bimester,
    semester,
    year_semester,
    fortnight,
    day_of_year,
    week_of_year
FROM data
ORDER BY date
```

Crie `treinamento_dbt/models/marts/dimensions/dim_date.yml`:

```yaml
version: 2

models:
  - name: dim_date
    description: "Dimens√£o conformed de data com todos os atributos de calend√°rio"
    columns:
      - name: date
        description: "Data (chave prim√°ria)"
        tests:
          - unique
          - not_null
      
      - name: year
        description: "Ano (YYYY)"
        tests:
          - not_null
      
      - name: month
        description: "M√™s (1-12)"
        tests:
          - not_null
      
      - name: is_weekend
        description: "Indicador de final de semana (0=N√£o, 1=Sim)"
      
      - name: quarter
        description: "Trimestre (1-4)"
```

### 13.3 Dimens√£o 2: dim_location

Crie `treinamento_dbt/models/marts/dimensions/dim_location.sql`:

```sql
-- =====================================================
-- Marts: Dimens√£o de Localiza√ß√£o
-- =====================================================

WITH data AS (
    SELECT *
    FROM {{ ref ('int_dim_location') }}
)

SELECT 
    location_id,
    sk_location_id
FROM data
ORDER BY location_id
```

Crie `treinamento_dbt/models/marts/dimensions/dim_location.yml`:

```yaml
version: 2

models:
  - name: dim_location
    description: "Dimens√£o de localiza√ß√µes (zonas de t√°xi)"
    columns:
      - name: location_id
        description: "ID da localiza√ß√£o (chave prim√°ria)"
        tests:
          - unique
          - not_null
      
      - name: sk_location_id
        description: "Surrogate key da localiza√ß√£o"
```

### 13.4 Dimens√£o 3: dim_vendor

Crie `treinamento_dbt/models/marts/dimensions/dim_vendor.sql`:

```sql
-- =====================================================
-- Marts: Dimens√£o de Fornecedor
-- =====================================================

WITH data AS (
    SELECT *
    FROM {{ ref ('int_dim_vendor') }}
)

SELECT 
    vendor_id,
    vendor_name,
    vendor_abbreviation
FROM data
ORDER BY vendor_id
```

Crie `treinamento_dbt/models/marts/dimensions/dim_vendor.yml`:

```yaml
version: 2

models:
  - name: dim_vendor
    description: "Dimens√£o de fornecedores de dados"
    columns:
      - name: vendor_id
        description: "ID do fornecedor (chave prim√°ria)"
        tests:
          - unique
          - not_null
      
      - name: vendor_name
        description: "Nome completo do fornecedor"
      
      - name: vendor_abbreviation
        description: "Sigla do fornecedor"
```

### 13.5 Dimens√£o 4: dim_payment_type

Crie `treinamento_dbt/models/marts/dimensions/dim_payment_type.sql`:

```sql
-- =====================================================
-- Marts: Dimens√£o de Tipo de Pagamento
-- =====================================================

WITH data AS (
    SELECT *
    FROM {{ ref ('int_dim_payment_type') }}
)

SELECT 
    payment_type,
    payment_type_name
FROM data
ORDER BY payment_type
```

Crie `treinamento_dbt/models/marts/dimensions/dim_payment_type.yml`:

```yaml
version: 2

models:
  - name: dim_payment_type
    description: "Dimens√£o de tipos de pagamento"
    columns:
      - name: payment_type
        description: "C√≥digo do tipo de pagamento (chave prim√°ria)"
        tests:
          - unique
          - not_null
      
      - name: payment_type_name
        description: "Descri√ß√£o do tipo de pagamento"
```

### 13.6 Dimens√£o 5: dim_rate_code

Crie `treinamento_dbt/models/marts/dimensions/dim_rate_code.sql`:

```sql
-- =====================================================
-- Marts: Dimens√£o de Rate Code
-- =====================================================

WITH data AS (
    SELECT *
    FROM {{ ref ('int_dim_rate_code') }}
)

SELECT 
    rate_code_id,
    rate_code_name,
    is_airport_trip
FROM data
ORDER BY rate_code_id
```

Crie `treinamento_dbt/models/marts/dimensions/dim_rate_code.yml`:

```yaml
version: 2

models:
  - name: dim_rate_code
    description: "Dimens√£o de c√≥digos de tarifa"
    columns:
      - name: rate_code_id
        description: "ID do c√≥digo de tarifa (chave prim√°ria)"
        tests:
          - unique
          - not_null
      
      - name: rate_code_name
        description: "Descri√ß√£o do c√≥digo de tarifa"
      
      - name: is_airport_trip
        description: "Indicador de viagem de/para aeroporto (1=JFK/Newark, 0=Outros)"
```

### 13.7 Dimens√£o 6: dim_time

Crie `treinamento_dbt/models/marts/dimensions/dim_time.sql`:

```sql
-- =====================================================
-- Marts: Dimens√£o de Tempo
-- =====================================================

WITH data AS (
    SELECT *
    FROM {{ ref ('int_dim_time') }}
)

SELECT 
    time,
    hour,
    minute,
    time_24h,
    period_of_day,
    is_rush_hour
FROM data
ORDER BY time
```

Crie `treinamento_dbt/models/marts/dimensions/dim_time.yml`:

```yaml
version: 2

models:
  - name: dim_time
    description: "Dimens√£o de tempo (hor√°rios do dia)"
    columns:
      - name: time
        description: "Hora e minuto (HH:MM:SS) - chave prim√°ria"
        tests:
          - unique
          - not_null
      
      - name: hour
        description: "Hora (0-23)"
      
      - name: minute
        description: "Minuto (0-59)"
      
      - name: time_24h
        description: "Hor√°rio formatado no padr√£o 24h (HH:mm) para visualiza√ß√µes"
      
      - name: period_of_day
        description: "Per√≠odo do dia (Madrugada, Manh√£, Tarde, Noite)"
      
      - name: is_rush_hour
        description: "Indica hor√°rio de pico (0=N√£o, 1=Sim)"
```

### 13.8 Executar Todas as Dimens√µes

```powershell
.\run_dbt.ps1 "run --select marts.dimensions"
```

‚úÖ **Resultado esperado:**
```
Completed successfully
6 of 6 OK created table model dbo_marts.dim_date
6 of 6 OK created table model dbo_marts.dim_location
6 of 6 OK created table model dbo_marts.dim_vendor
6 of 6 OK created table model dbo_marts.dim_payment_type
6 of 6 OK created table model dbo_marts.dim_rate_code
6 of 6 OK created table model dbo_marts.dim_time
```

### 13.9 Executar Testes de Qualidade

```powershell
.\run_dbt.ps1 "test --select marts.dimensions"
```

‚úÖ **Resultado esperado:** Todos os testes de `unique` e `not_null` devem passar.

### 13.10 Explorar as Dimens√µes Criadas

Agora que as dimens√µes finais foram materializadas como **tabelas**, vamos explorar os dados com consultas anal√≠ticas.

**üìÖ Consulta 1: Explorar dim_date (Dimens√£o de Data)**
```sql
-- Ver estrutura completa da dimens√£o
SELECT TOP 10 
    date,
    year,
    month,
    day_of_month,
    month_name,
    day_of_week_name,
    is_weekend,
    quarter,
    year_quarter,
    semester,
    week_of_year
FROM dbo_marts.dim_date
ORDER BY date DESC;

-- An√°lise de finais de semana por ano
SELECT 
    year,
    COUNT(*) as total_dias,
    SUM(is_weekend) as dias_fim_semana,
    COUNT(*) - SUM(is_weekend) as dias_uteis,
    CAST(SUM(is_weekend) * 100.0 / COUNT(*) AS DECIMAL(5,2)) as pct_fim_semana
FROM dbo_marts.dim_date
GROUP BY year
ORDER BY year;

-- Distribui√ß√£o de dias por trimestre
SELECT 
    year_quarter,
    COUNT(*) as total_dias,
    MIN(date) as primeira_data,
    MAX(date) as ultima_data
FROM dbo_marts.dim_date
GROUP BY year_quarter
ORDER BY year_quarter;
```

**üìç Consulta 2: Explorar dim_location (Dimens√£o de Localiza√ß√£o)**
```sql
-- Ver todas as localiza√ß√µes
SELECT 
    location_id,
    sk_location_id
FROM dbo_marts.dim_location
ORDER BY location_id;

-- Estat√≠sticas de localiza√ß√µes
SELECT 
    COUNT(*) as total_localizacoes,
    MIN(location_id) as menor_location_id,
    MAX(location_id) as maior_location_id
FROM dbo_marts.dim_location;
```

**üöñ Consulta 3: Explorar dim_vendor (Dimens√£o de Fornecedor)**
```sql
-- Ver detalhes de todos os vendors
SELECT 
    vendor_id,
    vendor_name,
    vendor_abbreviation
FROM dbo_marts.dim_vendor
ORDER BY vendor_id;
```

**üí≥ Consulta 4: Explorar dim_payment_type (Dimens√£o de Pagamento)**
```sql
-- Ver todos os tipos de pagamento dispon√≠veis
SELECT 
    payment_type,
    payment_type_name
FROM dbo_marts.dim_payment_type
ORDER BY payment_type;
```

**üí∞ Consulta 5: Explorar dim_rate_code (Dimens√£o de Tarifa)**
```sql
-- Ver todos os c√≥digos de tarifa
SELECT 
    rate_code_id,
    rate_code_name
FROM dbo_marts.dim_rate_code
ORDER BY rate_code_id;
```

**üïê Consulta 6: Explorar dim_time (Dimens√£o de Tempo)**
```sql
-- Ver primeiros e √∫ltimos hor√°rios
SELECT TOP 10
    time,
    hour,
    minute,
    period_of_day,
    is_rush_hour
FROM dbo_marts.dim_time
ORDER BY time;

-- An√°lise de hor√°rios de pico por per√≠odo do dia
SELECT 
    period_of_day,
    COUNT(*) as total_minutos,
    SUM(is_rush_hour) as minutos_pico,
    CAST(SUM(is_rush_hour) * 100.0 / COUNT(*) AS DECIMAL(5,2)) as pct_pico
FROM dbo_marts.dim_time
GROUP BY period_of_day
ORDER BY 
    CASE period_of_day
        WHEN 'Madrugada' THEN 1
        WHEN 'Manh√£' THEN 2
        WHEN 'Tarde' THEN 3
        WHEN 'Noite' THEN 4
    END;
```

**üîç Consulta 7: An√°lise Cross-Dimensional (Combinando Dimens√µes)**
```sql
-- Criar dataset combinado para an√°lise
-- Exemplo: Todas as combina√ß√µes de Data x Vendor x Tipo de Pagamento
SELECT TOP 100
    d.date,
    d.day_of_week_name,
    d.is_weekend,
    v.vendor_name,
    p.payment_type_name,
    r.rate_code_name
FROM dbo_marts.dim_date d
CROSS JOIN dbo_marts.dim_vendor v
CROSS JOIN dbo_marts.dim_payment_type p
CROSS JOIN dbo_marts.dim_rate_code r
WHERE d.year = 2013 AND d.month = 12  -- Filtro para reduzir resultados
ORDER BY d.date DESC, v.vendor_name, p.payment_type_name;

-- An√°lise de dias √∫teis vs finais de semana
SELECT 
    CASE WHEN d.is_weekend = 1 THEN 'Fim de Semana' ELSE 'Dia √ötil' END as tipo_dia,
    COUNT(DISTINCT d.date) as total_dias
FROM dbo_marts.dim_date d
GROUP BY d.is_weekend
ORDER BY d.is_weekend;
```

**üìä Consulta 8: Resumo Geral de Todas as Dimens√µes**
```sql
-- Contagem de registros em cada dimens√£o final
SELECT 'dim_date' as dimensao, COUNT(*) as total_registros, 'Tabela' as tipo
FROM dbo_marts.dim_date
UNION ALL
SELECT 'dim_location', COUNT(*), 'Tabela'
FROM dbo_marts.dim_location
UNION ALL
SELECT 'dim_vendor', COUNT(*), 'Tabela'
FROM dbo_marts.dim_vendor
UNION ALL
SELECT 'dim_payment_type', COUNT(*), 'Tabela'
FROM dbo_marts.dim_payment_type
UNION ALL
SELECT 'dim_rate_code', COUNT(*), 'Tabela'
FROM dbo_marts.dim_rate_code
UNION ALL
SELECT 'dim_time', COUNT(*), 'Tabela'
FROM dbo_marts.dim_time
ORDER BY total_registros DESC;
```

üí° **Insights das Dimens√µes:**
- **dim_date**: Base temporal completa para an√°lises hist√≥ricas
- **dim_location**: Permite an√°lise geogr√°fica das viagens
- **dim_vendor**: Compara√ß√£o entre fornecedores de dados
- **dim_payment_type**: An√°lise de prefer√™ncias de pagamento
- **dim_rate_code**: An√°lise de tipos de tarifas aplicadas
- **dim_time**: An√°lise de padr√µes hor√°rios e per√≠odos do dia

### 13.11 Validar no Fabric

```sql
-- Validar dimens√µes criadas no schema marts
SELECT COUNT(*) FROM dbt_marts.dim_date;          -- ~3.891 registros
SELECT COUNT(*) FROM dbt_marts.dim_location;      -- ~264 registros
SELECT COUNT(*) FROM dbt_marts.dim_vendor;        -- 2 registros
SELECT COUNT(*) FROM dbt_marts.dim_payment_type;  -- ~5 registros
SELECT COUNT(*) FROM dbt_marts.dim_rate_code;     -- ~7 registros
SELECT COUNT(*) FROM dbt_marts.dim_time;          -- 1.440 registros

-- Testar consulta anal√≠tica
SELECT TOP 10 
    d.date,
    d.day_of_week_name,
    d.is_weekend,
    v.vendor_name
FROM dbo_marts.dim_date d
CROSS JOIN dbo_marts.dim_vendor v
ORDER BY d.date DESC;
```

---

## üéØ 13 bis. CONSTRUINDO: Camada Marts - Facts (Gold)

Agora vamos criar a **tabela fato final** do Data Warehouse. Esta ser√° uma **tabela materializada** que referencia a camada intermediate.

### 13.12 Criar Tabela Fato - fct_taxi_trip (Incremental)

A tabela fato final materializa a view intermediate com **estrat√©gia incremental** para otimizar performance e reduzir tempo de processamento.

**üîë Estrat√©gia Incremental:**
- **Chave √∫nica:** `trip_id` (garante unicidade e permite merge)
- **Controle de carga:** `lpepPickupDatetime` (carrega apenas viagens novas)
- **Estrat√©gia:** `merge` (atualiza registros existentes se necess√°rio)
- **Benef√≠cios:** Processamento 10-100x mais r√°pido em cargas incrementais

**üìÑ Arquivo:** `treinamento_dbt/models/marts/facts/fct_taxi_trip.sql`

```sql
-- =====================================================
-- Fato: Viagens de T√°xi (Taxi Trip) - MART FINAL
-- =====================================================
-- Tabela fato final materializada que referencia a camada
-- intermediate com todos os relacionamentos j√° aplicados
-- Materializa√ß√£o: INCREMENTAL para performance otimizada
-- =====================================================
-- =====================================================
-- Fato: Viagens de T√°xi (Taxi Trip) - MART FINAL
-- =====================================================
-- Tabela fato final materializada que referencia a camada
-- intermediate com todos os relacionamentos j√° aplicados
-- Materializa√ß√£o: INCREMENTAL para performance otimizada
-- =====================================================

{{
    config(
        materialized='incremental',
        unique_key='trip_id',
        on_schema_change='sync_all_columns',
        incremental_strategy='delete+insert',
        tags=['fact', 'mart']
    )
}}

WITH int_fact AS (
    SELECT *
    FROM {{ ref('int_fct_taxi_trip') }}
    {% if is_incremental() %}
    -- Processa apenas viagens novas (baseado na data de pickup)
    WHERE pickup_date_fk > (SELECT MAX(pickup_date_fk) FROM {{ this }})
    {% endif %}
)

SELECT
    -- Chave Prim√°ria
    trip_id,
    
    -- Foreign Keys para Dimens√µes
    pickup_date_fk,
    dropoff_date_fk,
    pickup_time_fk,
    dropoff_time_fk,
    pickup_location_fk,      -- sk_location_id (surrogate key)
    dropoff_location_fk,     -- sk_location_id (surrogate key)
    vendor_fk,
    payment_type_fk,
    rate_code_fk,
    
    -- M√©tricas/Medidas
    fareAmount,
    extra,
    mtaTax,
    improvementSurcharge,
    tipAmount,
    tollsAmount,
    totalAmount,
    tripDistance,
    trip_duration_minutes,
    
    -- Atributos Descritivos (Degenerates)
    passengerCount,
    tripType,
    storeAndFwdFlag
    
FROM int_fact
```

**üí° Como funciona o Incremental:**

1. **Primeira execu√ß√£o (Full Load):**
   - `is_incremental()` retorna `false`
   - Processa TODAS as viagens da intermediate
   - Cria a tabela completa (~26M+ registros)
   - Tempo estimado: 5-15 minutos

2. **Execu√ß√µes subsequentes (Incremental):**
   - `is_incremental()` retorna `true`
   - Filtra: `WHERE lpepPickupDatetime > MAX(lpepPickupDatetime)`
   - Processa APENAS viagens novas
   - Tempo estimado: segundos/minutos (depende do volume novo)

3. **For√ßar Full Refresh (quando necess√°rio):**
   ```powershell
   .\run_dbt.ps1 "run --select fct_taxi_trip --full-refresh"
   ```

### 13.13 Documentar Tabela Fato - fct_taxi_trip.yml

**üìÑ Arquivo:** `treinamento_dbt/models/marts/facts/fct_taxi_trip.yml`

```yaml
version: 2

models:
  - name: fct_taxi_trip
    description: "Tabela fato contendo todas as viagens de t√°xi com refer√™ncias para dimens√µes conformed"
    config:
      tags: ['fact', 'mart']
    
    columns:
      # Chave Prim√°ria
      - name: trip_id
        description: "Chave prim√°ria √∫nica da viagem (surrogate key da staging)"
        tests:
          - unique
          - not_null
      
      # Foreign Keys - Data
      - name: pickup_date_fk
        description: "Foreign key para dim_date - data de coleta do passageiro"
        tests:
          - not_null
          - relationships:
              to: ref('dim_date')
              field: date
      
      - name: dropoff_date_fk
        description: "Foreign key para dim_date - data de entrega do passageiro"
        tests:
          - not_null
          - relationships:
              to: ref('dim_date')
              field: date
      
      # Foreign Keys - Tempo
      - name: pickup_time_fk
        description: "Foreign key para dim_time - hor√°rio de coleta do passageiro"
        tests:
          - not_null
          - relationships:
              to: ref('dim_time')
              field: time
      
      - name: dropoff_time_fk
        description: "Foreign key para dim_time - hor√°rio de entrega do passageiro"
        tests:
          - not_null
          - relationships:
              to: ref('dim_time')
              field: time
      
      # Foreign Keys - Localiza√ß√£o (USANDO SURROGATE KEY sk_location_id)
      - name: pickup_location_fk
        description: "Foreign key para dim_location - sk_location_id da localiza√ß√£o de coleta"
        tests:
          - not_null
          - relationships:
              to: ref('dim_location')
              field: sk_location_id
      
      - name: dropoff_location_fk
        description: "Foreign key para dim_location - sk_location_id da localiza√ß√£o de entrega"
        tests:
          - not_null
          - relationships:
              to: ref('dim_location')
              field: sk_location_id
      
      # Foreign Keys - Outras Dimens√µes
      - name: vendor_fk
        description: "Foreign key para dim_vendor - fornecedor do servi√ßo"
        tests:
          - not_null
          - relationships:
              to: ref('dim_vendor')
              field: vendor_id
      
      - name: payment_type_fk
        description: "Foreign key para dim_payment_type - tipo de pagamento"
        tests:
          - not_null
          - relationships:
              to: ref('dim_payment_type')
              field: payment_type
      
      - name: rate_code_fk
        description: "Foreign key para dim_rate_code - c√≥digo de tarifa"
        tests:
          - not_null
          - relationships:
              to: ref('dim_rate_code')
              field: rate_code_id
      
      # M√©tricas/Medidas
      - name: fareAmount
        description: "Valor base da tarifa calculado pelo tax√≠metro"
        tests:
          - not_null
      
      - name: totalAmount
        description: "Valor total cobrado ao passageiro (soma de todos os valores)"
        tests:
          - not_null
      
      - name: tripDistance
        description: "Dist√¢ncia da viagem em milhas reportada pelo tax√≠metro"
        tests:
          - not_null
      
      - name: trip_duration_minutes
        description: "Dura√ß√£o da viagem calculada em minutos (dropoff - pickup)"
        tests:
          - not_null
      
      # Timestamps Originais
      - name: lpepPickupDatetime
        description: "Data e hora exata em que o tax√≠metro foi acionado"
        tests:
          - not_null
      
      - name: lpepDropoffDatetime
        description: "Data e hora exata em que o tax√≠metro foi desligado"
        tests:
          - not_null
```

### 13.14 Executar e Testar a Tabela Fato

**üéØ Build completo (dimens√µes + fato):**
```powershell
.\run_dbt.ps1 "build"
```

**üéØ Executar apenas a fato (primeira carga - FULL):**
```powershell
.\run_dbt.ps1 "run --select fct_taxi_trip"
```

**üîÑ Executar carga incremental (cargas subsequentes):**
```powershell
# Comando normal - automaticamente detecta que √© incremental
.\run_dbt.ps1 "run --select fct_taxi_trip"

# Sa√≠da esperada:
# 1 of 1 START sql incremental model dbo_marts.fct_taxi_trip
# 1 of 1 OK created sql incremental model dbo_marts.fct_taxi_trip [merge in 2s]
```

**üîÑ For√ßar Full Refresh (recriar tabela completa):**
```powershell
# Use quando houver mudan√ßas na estrutura ou l√≥gica de neg√≥cio
.\run_dbt.ps1 "run --select fct_taxi_trip --full-refresh"

# Sa√≠da esperada:
# 1 of 1 START sql incremental model dbo_marts.fct_taxi_trip
# 1 of 1 OK created sql incremental model dbo_marts.fct_taxi_trip [INSERT 0 in 120s]
```

**üß™ Testar integridade referencial:**
```powershell
.\run_dbt.ps1 "test --select fct_taxi_trip"
```

**üìä Validar Performance do Incremental:**
```sql
-- No Fabric, verificar o MAX(lpepPickupDatetime) atual
SELECT 
    COUNT(*) as total_viagens,
    MIN(lpepPickupDatetime) as primeira_viagem,
    MAX(lpepPickupDatetime) as ultima_viagem
FROM dbo_marts.fct_taxi_trip;

-- Simular: Se novos dados fossem carregados na staging ap√≥s '2019-12-31',
-- o incremental processaria APENAS essas novas viagens
```

### 13.15 Valida√ß√£o e An√°lises da Tabela Fato

**üîç Consulta 1: Volume de Dados**
```sql
-- Verificar quantidade de viagens
SELECT COUNT(*) as total_viagens
FROM dbo_marts.fct_taxi_trip;
-- Esperado: ~26M+ viagens

-- Distribui√ß√£o por vendor
SELECT 
    v.vendor_name,
    COUNT(*) as total_viagens,
    ROUND(AVG(f.totalAmount), 2) as receita_media,
    ROUND(AVG(f.tripDistance), 2) as distancia_media
FROM dbo_marts.fct_taxi_trip f
INNER JOIN dbo_marts.dim_vendor v ON f.vendor_fk = v.vendor_id
GROUP BY v.vendor_name;
```

**üîç Consulta 2: An√°lise Temporal**
```sql
-- Viagens por per√≠odo do dia
SELECT 
    t.period_of_day,
    COUNT(*) as total_viagens,
    ROUND(AVG(f.totalAmount), 2) as receita_media
FROM dbo_marts.fct_taxi_trip f
INNER JOIN dbo_marts.dim_time t ON f.pickup_time_fk = t.time
GROUP BY t.period_of_day
ORDER BY 
    CASE t.period_of_day
        WHEN 'Madrugada' THEN 1
        WHEN 'Manh√£' THEN 2
        WHEN 'Tarde' THEN 3
        WHEN 'Noite' THEN 4
    END;

-- Viagens por dia da semana
SELECT 
    d.day_of_week_name,
    d.is_weekend,
    COUNT(*) as total_viagens,
    ROUND(AVG(f.totalAmount), 2) as receita_media
FROM dbo_marts.fct_taxi_trip f
INNER JOIN dbo_marts.dim_date d ON f.pickup_date_fk = d.date
GROUP BY d.day_of_week_name, d.is_weekend, d.day_of_week
ORDER BY d.day_of_week;
```

**üîç Consulta 3: An√°lise Geogr√°fica**
```sql
-- Top 10 localiza√ß√µes de origem com mais viagens
SELECT TOP 10
    l.location_id,
    COUNT(*) as total_viagens,
    ROUND(AVG(f.totalAmount), 2) as receita_media,
    ROUND(AVG(f.tripDistance), 2) as distancia_media
FROM dbo_marts.fct_taxi_trip f
INNER JOIN dbo_marts.dim_location l ON f.pickup_location_fk = l.sk_location_id
GROUP BY l.location_id
ORDER BY total_viagens DESC;

-- An√°lise de rotas mais comuns (origem ‚Üí destino)
SELECT TOP 20
    l_pickup.location_id as location_origem,
    l_dropoff.location_id as location_destino,
    COUNT(*) as total_viagens,
    ROUND(AVG(f.totalAmount), 2) as receita_media
FROM dbo_marts.fct_taxi_trip f
INNER JOIN dbo_marts.dim_location l_pickup ON f.pickup_location_fk = l_pickup.sk_location_id
INNER JOIN dbo_marts.dim_location l_dropoff ON f.dropoff_location_fk = l_dropoff.sk_location_id
GROUP BY l_pickup.location_id, l_dropoff.location_id
ORDER BY total_viagens DESC;
```

**üîç Consulta 4: An√°lise de Pagamento e Tarifas**
```sql
-- Distribui√ß√£o por tipo de pagamento
SELECT 
    p.payment_type_name,
    COUNT(*) as total_viagens,
    ROUND(AVG(f.totalAmount), 2) as receita_media,
    ROUND(AVG(f.tipAmount), 2) as gorjeta_media
FROM dbo_marts.fct_taxi_trip f
INNER JOIN dbo_marts.dim_payment_type p ON f.payment_type_fk = p.payment_type
GROUP BY p.payment_type_name
ORDER BY total_viagens DESC;

-- An√°lise por tipo de tarifa
SELECT 
    r.rate_code_name,
    COUNT(*) as total_viagens,
    ROUND(AVG(f.totalAmount), 2) as receita_media
FROM dbo_marts.fct_taxi_trip f
INNER JOIN dbo_marts.dim_rate_code r ON f.rate_code_fk = r.rate_code_id
GROUP BY r.rate_code_name
ORDER BY total_viagens DESC;
```

**üîç Consulta 5: An√°lise Completa (Star Schema Join)**
```sql
-- An√°lise completa: Viagens de fim de semana √† noite
SELECT TOP 100
    d.date,
    d.day_of_week_name,
    t.time_24h,
    t.period_of_day,
    l_pickup.location_id as location_origem,
    l_dropoff.location_id as location_destino,
    v.vendor_name,
    p.payment_type_name,
    r.rate_code_name,
    f.totalAmount,
    f.tripDistance,
    f.trip_duration_minutes,
    f.passengerCount
FROM dbo_marts.fct_taxi_trip f
INNER JOIN dbo_marts.dim_date d ON f.pickup_date_fk = d.date
INNER JOIN dbo_marts.dim_time t ON f.pickup_time_fk = t.time
INNER JOIN dbo_marts.dim_location l_pickup ON f.pickup_location_fk = l_pickup.sk_location_id
INNER JOIN dbo_marts.dim_location l_dropoff ON f.dropoff_location_fk = l_dropoff.sk_location_id
INNER JOIN dbo_marts.dim_vendor v ON f.vendor_fk = v.vendor_id
INNER JOIN dbo_marts.dim_payment_type p ON f.payment_type_fk = p.payment_type
INNER JOIN dbo_marts.dim_rate_code r ON f.rate_code_fk = r.rate_code_id
WHERE d.is_weekend = 1
  AND t.period_of_day = 'Noite'
ORDER BY f.totalAmount DESC;
```

**üîç Consulta 6: M√©tricas de Neg√≥cio (KPIs)**
```sql
-- KPIs principais do neg√≥cio de t√°xi
SELECT 
    COUNT(*) as total_viagens,
    COUNT(DISTINCT pickup_date_fk) as dias_operacionais,
    ROUND(SUM(totalAmount), 2) as receita_total,
    ROUND(AVG(totalAmount), 2) as ticket_medio,
    ROUND(AVG(tripDistance), 2) as distancia_media_milhas,
    ROUND(AVG(trip_duration_minutes), 2) as duracao_media_minutos,
    ROUND(AVG(passengerCount), 2) as passageiros_medio,
    ROUND(AVG(tipAmount), 2) as gorjeta_media,
    ROUND(SUM(tollsAmount), 2) as pedagios_total
FROM dbo_marts.fct_taxi_trip;
```

üí° **Arquitetura Star Schema Completa:**
```
                 dim_date
                     |
                     |
    dim_vendor ---- fct_taxi_trip ---- dim_location (pickup)
                     |                           |
                     |                           |
              dim_payment_type            dim_location (dropoff)
                     |
                     |
              dim_rate_code
                     |
                     |
                 dim_time
```

‚úÖ **Checkpoint:** Data Warehouse dimensional criado! Todas as 6 dimens√µes est√£o prontas, validadas e com dados explor√°veis. A tabela fato est√° materializada com ~26M+ viagens e todos os testes de integridade referencial passando.

---

## üìñ 14. Documenta√ß√£o e Lineage

### 14.1 Gerar Documenta√ß√£o

O dbt gera documenta√ß√£o autom√°tica com lineage (linhagem) dos dados:

```powershell
.\run_dbt.ps1 "docs generate"
```

### 14.2 Visualizar Documenta√ß√£o

```powershell
.\run_dbt.ps1 "docs serve"
```

Isso abrir√° um servidor local (geralmente `http://localhost:8080`) com:
- üìä **Lineage Graph** - Visualiza√ß√£o do fluxo de dados
- üìù **Documenta√ß√£o** - Descriptions de todos os modelos
- ‚úÖ **Testes** - Status de todos os testes
- üìà **M√©tricas** - Tempo de execu√ß√£o, rows processadas

### 14.3 Navegar na Documenta√ß√£o

1. **Project** ‚Üí Navegue pelos modelos
2. **Database** ‚Üí Veja os objetos criados no Fabric
3. **Graph** ‚Üí Explore o lineage visual:
   - Verde = Sources
   - Azul = Models
   - Linhas = Depend√™ncias

### 14.4 Compartilhar Documenta√ß√£o

Para compartilhar a documenta√ß√£o com o time:

**Op√ß√£o 1 - Hospedar em GitHub Pages:**
1. Gerar docs: `dbt docs generate`
2. Copiar `/target/index.html` e `/target/catalog.json`
3. Publicar em GitHub Pages

**Op√ß√£o 2 - Usar dbt Cloud:**
- Upload autom√°tico da documenta√ß√£o
- Acesso via web para todo o time

---

## üõ†Ô∏è 15. Comandos √öteis do Dia a Dia

### 15.1 Execu√ß√£o Seletiva

```powershell
# Executar apenas staging
.\run_dbt.ps1 "run --select staging"

# Executar apenas intermediate
.\run_dbt.ps1 "run --select intermediate"

# Executar apenas marts
.\run_dbt.ps1 "run --select marts"

# Executar um modelo espec√≠fico
.\run_dbt.ps1 "run --select dim_date"

# Executar um modelo e suas depend√™ncias
.\run_dbt.ps1 "run --select +dim_date"

# Executar um modelo e seus dependentes
.\run_dbt.ps1 "run --select dim_date+"

# Executar modelos modificados
.\run_dbt.ps1 "run --select state:modified+"
```

### 15.2 Testes

```powershell
# Executar todos os testes
.\run_dbt.ps1 test

# Testar apenas staging
.\run_dbt.ps1 "test --select staging"

# Testar um modelo espec√≠fico
.\run_dbt.ps1 "test --select dim_date"
```

### 15.3 Compila√ß√£o e Debug

```powershell
# Compilar sem executar (ver SQL gerado)
.\run_dbt.ps1 compile

# Ver SQL compilado de um modelo
.\run_dbt.ps1 "compile --select dim_date"
# Resultado em: treinamento_dbt/target/compiled/

# Debug de conex√£o
.\run_dbt.ps1 debug
```

### 15.4 Freshness de Sources

```powershell
# Verificar atualiza√ß√£o das sources
.\run_dbt.ps1 "source freshness"
```

### 15.5 Limpeza

```powershell
# Limpar diret√≥rio target
.\run_dbt.ps1 clean
```

---

## üîç 16. Troubleshooting

### 16.1 Erros de Conex√£o

**Erro: "environment variable 'XXX' not found"**
```
Solu√ß√£o:
1. Verifique se o arquivo .env existe na raiz do projeto
2. Confirme que est√° usando os scripts run_dbt.ps1 ou run_dbt.sh
3. Verifique se as vari√°veis est√£o definidas corretamente no .env
```

**Erro: "Unable to connect to database"**
```
Solu√ß√£o:
1. Verifique se FABRIC_SERVER est√° correto no.env
2. Para fabric_local: Execute 'az login' e verifique a conta ativa com 'az account show'
3. Para fabric: Valide tenant_id, client_id e client_secret
4. Teste conectividade com Azure Data Studio ou outra ferramenta SQL
5. Confirme que voc√™ tem permiss√µes no workspace Fabric
```

**Erro: "Authentication failed"**
```
Solu√ß√£o:
1. Para fabric_local: Token pode ter expirado, execute 'az login' novamente
2. Para fabric: Verifique as credenciais do Service Principal
3. Confirme que o Service Principal tem permiss√µes Admin/Member no workspace
```

### 16.2 Erros de Execu√ß√£o

**Erro: "Compilation Error" ou "Syntax Error"**
```
Solu√ß√£o:
1. Execute 'dbt compile' para ver o SQL gerado
2. Verifique se h√° erros de sintaxe SQL no modelo
3. Confirme que todas as refer√™ncias {{ ref() }} e {{ source() }} est√£o corretas
4. Verifique se os nomes de colunas correspondem aos dados reais
```

**Erro: "Relation does not exist"**
```
Solu√ß√£o:
1. Verifique se a source ou modelo referenciado existe
2. Execute os modelos upstream primeiro (ex: staging antes de intermediate)
3. Confirme que o schema est√° correto no profiles.yml
```

**Erro: "Column not found"**
```
Solu√ß√£o:
1. Verifique se a coluna existe na tabela fonte
2. Confirme se o nome da coluna est√° correto (case-sensitive)
3. Execute SELECT * na source para ver estrutura real
```

### 16.3 Erros de Performance

**Modelo muito lento**
```
Solu√ß√£o:
1. Considere usar materializa√ß√£o 'table' ao inv√©s de 'view'
2. Para tabelas grandes, use materializa√ß√£o 'incremental'
3. Adicione √≠ndices nas tabelas finais (ap√≥s o dbt run)
4. Revise queries para otimizar joins e filtros
```

**Erro: "Timeout" ou "Memory exceeded"**
```
Solu√ß√£o:
1. Aumente o valor de DBT_THREADS no .env (ex: DBT_THREADS=2)
2. Execute modelos em lotes menores com --select
3. Use materializa√ß√£o incremental para tabelas grandes
```

### 16.4 Erros de Testes

**Teste failing: "unique" ou "not_null"**
```
Solu√ß√£o:
1. Execute query direto no Fabric para investigar:
   SELECT column_name, COUNT(*) 
   FROM schema.table 
   GROUP BY column_name 
   HAVING COUNT(*) > 1
2. Adicione filtros ou transforma√ß√µes no modelo para corrigir dados
3. Se esperado, remova ou ajuste o teste
```

### 16.5 Dicas de Debug

**Ver SQL compilado:**
```powershell
.\run_dbt.ps1 compile
# Resultado em: treinamento_dbt/target/compiled/...
```

**Executar apenas um modelo para testar:**
```powershell
.\run_dbt.ps1 "run --select dim_date --full-refresh"
```

**Ver logs detalhados:**
```powershell
.\run_dbt.ps1 "run --select dim_date --debug"
```

**Verificar depend√™ncias de um modelo:**
```powershell
.\run_dbt.ps1 "list --select +dim_date+"
```

---

## üìö 17. Recursos e Pr√≥ximos Passos

### 17.1 Documenta√ß√£o Oficial

- [dbt Documentation](https://docs.getdbt.com/) - Documenta√ß√£o completa do dbt
- [dbt-fabric Adapter](https://docs.getdbt.com/docs/core/connect-data-platform/fabric-setup) - Espec√≠fico para Microsoft Fabric
- [Microsoft Fabric Docs](https://learn.microsoft.com/en-us/fabric/) - Documenta√ß√£o do Fabric
- [dbt Best Practices](https://docs.getdbt.com/guides/best-practices) - Boas pr√°ticas oficiais

### 17.2 Evolu√ß√£o do Projeto

**Pr√≥ximos passos recomendados:**

1. **‚úÖ Criar Tabelas Fato** (CONCLU√çDO)
   - ‚úÖ `fct_taxi_trip` - Fato de viagens com m√©tricas
   - ‚úÖ Joins com todas as dimens√µes criadas usando surrogate keys
   - ‚úÖ Separa√ß√£o de camadas intermediate (VIEW) e marts (TABLE)
   - ‚úÖ **Materializa√ß√£o incremental implementada** (otimiza performance 10-100x)
   - üí° Beneficio: Cargas incrementais levam segundos ao inv√©s de minutos

2. **üìä Implementar M√©tricas**
   - Criar m√©tricas reutiliz√°veis com dbt metrics
   - Total de viagens, receita m√©dia, dist√¢ncia total, etc.

3. **üì∏ Adicionar Snapshots (SCD Type 2)**
   - Rastrear mudan√ßas hist√≥ricas em dimens√µes
   - Manter hist√≥rico de altera√ß√µes

4. **üîç Testes Customizados**
   - Criar macros de testes espec√≠ficos do neg√≥cio
   - Valida√ß√µes de regras de neg√≥cio complexas

5. **ü§ñ CI/CD com GitHub Actions**
   - Automatizar execu√ß√£o do dbt em PRs
   - Deploy autom√°tico em produ√ß√£o
   - Testes autom√°ticos antes de merge

6. **üìà Monitoring e Alertas**
   - Configurar alertas de freshness
   - Monitorar tempo de execu√ß√£o
   - Dashboard de quality checks

7. **üîê Governan√ßa de Dados**
   - Adicionar tags para classifica√ß√£o
   - Documentar ownership de modelos
   - Implementar access control

### 17.3 Boas Pr√°ticas

**Sempre fa√ßa:**
- ‚úÖ Versionar c√≥digo no Git com commits descritivos
- ‚úÖ Nunca commitar credenciais (usar `.env`)
- ‚úÖ Documentar todos os modelos com `description`
- ‚úÖ Implementar testes de qualidade em campos cr√≠ticos
- ‚úÖ Usar nomenclatura consistente (prefixos: stg_, int_, dim_, fct_)
- ‚úÖ Revisar c√≥digo via Pull Requests
- ‚úÖ Executar `dbt test` antes de fazer merge
- ‚úÖ Manter `README.md` atualizado

**Evite:**
- ‚ùå Hardcodear valores (usar vari√°veis e macros)
- ‚ùå Criar depend√™ncias circulares entre modelos
- ‚ùå Modelos muito complexos (quebrar em intermediate)
- ‚ùå Commit direto em main/master
- ‚ùå Pular testes por "falta de tempo"

### 17.4 Estrutura de Branches Recomendada

```text
main/master     ‚Üí Produ√ß√£o (fabric com Service Principal)
develop         ‚Üí Desenvolvimento (fabric_local com az login)
feature/*       ‚Üí Features individuais
hotfix/*        ‚Üí Corre√ß√µes urgentes
```

### 17.5 Template de Commit

```bash
# Formato sugerido
<tipo>: <descri√ß√£o curta>

<descri√ß√£o detalhada opcional>

Tipos:
- feat: Nova feature
- fix: Corre√ß√£o de bug
- docs: Documenta√ß√£o
- refactor: Refatora√ß√£o
- test: Adicionar testes
- chore: Manuten√ß√£o

Exemplo:
feat: adicionar dimens√£o de tempo com per√≠odo do dia

- Criar int_dim_time com gera√ß√£o de hor√°rios
- Adicionar dim_time no marts
- Incluir atributos de per√≠odo e hor√°rio de pico
```

### 17.6 Comunidade e Suporte

- [dbt Community Slack](https://www.getdbt.com/community/join-the-community/) - Comunidade ativa
- [dbt Discourse](https://discourse.getdbt.com/) - F√≥rum de discuss√µes
- [GitHub dbt-core](https://github.com/dbt-labs/dbt-core) - Issues e contribui√ß√µes
- [GitHub dbt-fabric](https://github.com/microsoft/dbt-fabric) - Adapter espec√≠fico

---

## üéì 18. Observa√ß√µes Finais

### ‚ú® O que voc√™ construiu:

Este projeto implementa um **Data Warehouse dimensional completo** seguindo as melhores pr√°ticas de engenharia de dados:

**üìä Arquitetura:**
- ü•â **Camada Bronze (Staging)**: 1 source, 1 modelo de ingest√£o
- ü•à **Camada Silver (Intermediate)**: 6 dimens√µes intermedi√°rias + 1 tabela fato intermedi√°ria
- ü•á **Camada Gold (Marts)**: 6 dimens√µes finais + 1 tabela fato otimizada (~26M+ viagens)

**üîß Infraestrutura:**
- ‚òÅÔ∏è Totalmente integrado com **Microsoft Fabric**
- üîê Seguran√ßa via vari√°veis de ambiente e `.gitignore`
- üöÄ Scripts automatizados para facilitar execu√ß√£o
- üìù Documenta√ß√£o completa com lineage autom√°tico
- ‚úÖ Testes de qualidade implementados

**üìà Boas Pr√°ticas:**
- Arquitetura Medallion (Bronze/Silver/Gold)
- Separa√ß√£o de responsabilidades por camada
- Nomenclatura consistente e descritiva
- C√≥digo versionado e documentado
- Estrat√©gias de materializa√ß√£o otimizadas

### üìÅ Arquivos Criados no Projeto:

**Camada Staging (Bronze):**
- `models/staging/lakehouse/_lakehouse_sources.yml` - Defini√ß√£o da fonte de dados do lakehouse
- `models/staging/lakehouse/stg_lakehouse__taxi.sql` - Modelo staging para dados de t√°xi

**Camada Intermediate (Silver):**
- `models/intermediate/lakehouse/int_dim_date.sql` - Dimens√£o data intermedi√°ria
- `models/intermediate/lakehouse/int_dim_location.sql` - Dimens√£o localiza√ß√£o intermedi√°ria  
- `models/intermediate/lakehouse/int_dim_payment_type.sql` - Dimens√£o tipo de pagamento intermedi√°ria
- `models/intermediate/lakehouse/int_dim_rate_code.sql` - Dimens√£o c√≥digo de tarifa intermedi√°ria
- `models/intermediate/lakehouse/int_dim_time.sql` - Dimens√£o tempo intermedi√°ria
- `models/intermediate/lakehouse/int_dim_vendor.sql` - Dimens√£o fornecedor intermedi√°ria
- `models/intermediate/lakehouse/int_fct_taxi_trip.sql` - Fato viagem de t√°xi intermedi√°ria

**Camada Marts (Gold) - Dimens√µes:**
- `models/marts/dimensions/dim_date.sql` + `dim_date.yml` - Dimens√£o data com documenta√ß√£o e testes
- `models/marts/dimensions/dim_location.sql` + `dim_location.yml` - Dimens√£o localiza√ß√£o com documenta√ß√£o e testes
- `models/marts/dimensions/dim_payment_type.sql` + `dim_payment_type.yml` - Dimens√£o tipo de pagamento com documenta√ß√£o e testes
- `models/marts/dimensions/dim_rate_code.sql` + `dim_rate_code.yml` - Dimens√£o c√≥digo de tarifa com documenta√ß√£o e testes
- `models/marts/dimensions/dim_time.sql` + `dim_time.yml` - Dimens√£o tempo com documenta√ß√£o e testes
- `models/marts/dimensions/dim_vendor.sql` + `dim_vendor.yml` - Dimens√£o fornecedor com documenta√ß√£o e testes

**Camada Marts (Gold) - Fatos:**
- `models/marts/facts/fct_taxi_trip.sql` + `fct_taxi_trip.yml` - Fato viagem de t√°xi com documenta√ß√£o e testes

**Configura√ß√£o e Infraestrutura:**
- `requirements.txt` - Depend√™ncias Python do projeto
- `run_dbt.ps1` - Script PowerShell para execu√ß√£o do dbt
- `run_dbt.sh` - Script Bash para execu√ß√£o do dbt
- `treinamento_dbt/dbt_project.yml` - Configura√ß√£o do projeto dbt
- `treinamento_dbt/profiles.yml` - Perfis de conex√£o com Microsoft Fabric

**Total:** 1 source + 1 staging + 7 intermediate + 6 dimensions + 1 fact = **16 modelos SQL** + **7 arquivos YAML de documenta√ß√£o/testes**

### üéØ Benef√≠cios Alcan√ßados:

1. **Replicabilidade** - Qualquer pessoa pode seguir este README e recriar o projeto
2. **Manutenibilidade** - C√≥digo organizado e documentado
3. **Escalabilidade** - Arquitetura preparada para crescer
4. **Qualidade** - Testes garantem integridade dos dados
5. **Performance** - Materializa√ß√µes otimizadas por caso de uso
6. **Colabora√ß√£o** - Documenta√ß√£o facilita trabalho em equipe

### üöÄ Pr√≥xima Jornada:

Voc√™ agora tem uma **base s√≥lida** para:
- Adicionar novas fontes de dados
- Criar tabelas fato complexas
- Implementar m√©tricas de neg√≥cio
- Automatizar com CI/CD
- Escalar para produ√ß√£o

---

**üéâ Parab√©ns!** Voc√™ completou o treinamento de dbt com Microsoft Fabric.

Este projeto est√° pronto para ser usado como:
- üìö **Material de treinamento** para novos membros do time
- üèóÔ∏è **Template** para novos projetos dbt
- üìñ **Refer√™ncia** de boas pr√°ticas
- üéØ **Base** para evolu√ß√£o cont√≠nua

**Continue aprendendo e construindo! üöÄ**

---

_√öltima atualiza√ß√£o: 12 de Fevereiro de 2026_
