# Treinamento dbt com Microsoft Fabric

Este documento descreve, de forma **coerente, l√≥gica e replic√°vel**, o passo a passo para cria√ß√£o e execu√ß√£o de um projeto de **dbt** integrado ao **Microsoft Fabric**, servindo como base para treinamentos e desenvolvimento.

---

## üìã 1. Pr√©-requisitos

Antes de iniciar, garanta que os seguintes itens estejam dispon√≠veis:

- Sistema operacional: **Linux, macOS ou Windows**
- **Python 3.8 ou superior**
- Acesso a um workspace do **Microsoft Fabric**
- **Azure CLI** instalado e configurado (para autentica√ß√£o CLI)
- **Microsoft ODBC Driver 18** ou superior para SQL Server
- **Git** para versionamento

### Instala√ß√£o do Azure CLI

- Windows: [Download Azure CLI](https://learn.microsoft.com/pt-br/cli/azure/install-azure-cli-windows)
- Linux/macOS: 
  ```bash
  curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
  ```

### Instala√ß√£o do ODBC Driver

- Windows: [Download ODBC Driver 18](https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server)
- Linux: [Instru√ß√µes de instala√ß√£o](https://learn.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server)

---

## üöÄ 2. Setup do Ambiente

### 2.1 Cria√ß√£o do diret√≥rio do projeto

```bash
mkdir treinamento-dbt
cd treinamento-dbt
```

### 2.2 Cria√ß√£o e ativa√ß√£o do ambiente virtual

**Criar ambiente virtual:**
```bash
python -m venv .venv
```

**Ativar ambiente virtual:**

- **Linux / macOS:**
  ```bash
  source .venv/bin/activate
  ```

- **Windows (PowerShell):**
  ```powershell
  .venv\Scripts\Activate.ps1
  ```

- **Windows (CMD):**
  ```cmd
  .venv\Scripts\activate.bat
  ```

### 2.3 Instala√ß√£o das depend√™ncias

Crie um arquivo `requirements.txt` na raiz do projeto:

```txt
# =====================================================
# DBT Dependencies
# =====================================================
dbt-core==1.11.2
dbt-fabric==1.9.3
dbt-sqlserver==1.9.0
python-dotenv==1.2.1
```

**Instale as depend√™ncias:**

```bash
pip install -r requirements.txt
```

**Verificar instala√ß√£o:**
```bash
dbt --version
```

---

## üìÅ 3. Cria√ß√£o do Projeto dbt

### 3.1 Inicializar projeto dbt

```bash
dbt init treinamento_dbt
```

Durante a inicializa√ß√£o, o dbt far√° algumas perguntas:
- **Which database would you like to use?** Escolha o n√∫mero correspondente ao **fabric**
- As demais configura√ß√µes ser√£o feitas via `profiles.yml`

### 3.2 Estrutura do projeto criada

```text
treinamento-dbt/
‚îú‚îÄ‚îÄ .venv/                    # Ambiente virtual Python
‚îú‚îÄ‚îÄ .env                      # Vari√°veis de ambiente (N√ÉO versionar)
‚îú‚îÄ‚îÄ .gitignore               # Arquivos ignorados pelo Git
‚îú‚îÄ‚îÄ requirements.txt         # Depend√™ncias Python
‚îú‚îÄ‚îÄ run_dbt.ps1             # Script PowerShell para executar DBT
‚îú‚îÄ‚îÄ run_dbt.sh              # Script Bash para executar DBT
‚îú‚îÄ‚îÄ PERFIS_DBT.md           # Documenta√ß√£o dos perfis
‚îî‚îÄ‚îÄ treinamento_dbt/        # Pasta do projeto DBT
    ‚îú‚îÄ‚îÄ dbt_project.yml     # Configura√ß√£o do projeto
    ‚îú‚îÄ‚îÄ profiles.yml        # Configura√ß√£o de conex√µes
    ‚îú‚îÄ‚îÄ models/             # Modelos SQL
    ‚îú‚îÄ‚îÄ tests/              # Testes customizados
    ‚îú‚îÄ‚îÄ macros/             # Macros Jinja
    ‚îú‚îÄ‚îÄ seeds/              # Arquivos CSV para carga
    ‚îú‚îÄ‚îÄ snapshots/          # Snapshots de dados
    ‚îî‚îÄ‚îÄ analyses/           # An√°lises ad-hoc
```

---

## üîê 4. Configura√ß√£o de Vari√°veis de Ambiente

### 4.1 Criar arquivo `.env`

Crie um arquivo `.env` na **raiz do projeto** (fora da pasta `treinamento_dbt`):

```env
# =====================================================
# DBT Database Configuration
# =====================================================

# ========== PERFIL ATIVO ==========
# Escolha: 'fabric_local' (desenvolvimento) ou 'fabric' (produ√ß√£o)
DBT_TARGET=fabric_local

# ========== CONFIGURA√á√ÉO FABRIC LOCAL (Desenvolvimento com az login) ==========
# Use este perfil para desenvolvimento com suas credenciais Azure
# Requisito: Execute 'az login' antes de usar o DBT
FABRIC_SERVER=seu-workspace.datawarehouse.fabric.microsoft.com
FABRIC_DATABASE=DataWarehouseTreinamento
FABRIC_SCHEMA=dbo

# ========== CONFIGURA√á√ÉO FABRIC (Service Principal - Produ√ß√£o/CI/CD) ==========
# Use este perfil para automa√ß√£o e pipelines
# Descomente e configure para usar Fabric em produ√ß√£o:
# FABRIC_TENANT_ID=your-tenant-id
# FABRIC_CLIENT_ID=your-client-id
# FABRIC_CLIENT_SECRET=your-client-secret

# ========== CONFIGURA√á√ïES GERAIS ==========
ODBC_DRIVER=ODBC Driver 18 for SQL Server
DBT_THREADS=4
```

> **‚ö†Ô∏è IMPORTANTE:** Nunca versione o arquivo `.env` com credenciais reais!

### 4.2 Criar arquivo `.env.example`

Crie um template sem credenciais para versionamento:

```env
# =====================================================
# DBT Database Configuration - TEMPLATE
# =====================================================
# Copie este arquivo para .env e preencha com suas credenciais

DBT_TARGET=fabric_local

# Fabric Configuration (Development)
FABRIC_SERVER=your-workspace.datawarehouse.fabric.microsoft.com
FABRIC_DATABASE=DataWarehouseTreinamento
FABRIC_SCHEMA=dbo

# Fabric Configuration (Production - Service Principal)
# FABRIC_TENANT_ID=your-tenant-id
# FABRIC_CLIENT_ID=your-client-id
# FABRIC_CLIENT_SECRET=your-client-secret

ODBC_DRIVER=ODBC Driver 18 for SQL Server
DBT_THREADS=4
```

### 4.3 Configurar `.gitignore`

Crie ou atualize o arquivo `.gitignore` na raiz:

```gitignore
# DBT
target/
dbt_packages/
logs/
dbt_modules/

# Python
__pycache__/
*.py[cod]
*$py.class
.Python
.venv/
venv/
ENV/
env/

# Ambiente e Credenciais
.env
.env.local
.env.*.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Outros
*.log
```

---

## ‚öôÔ∏è 5. Configura√ß√£o do Profiles do dbt

### 5.1 Criar `treinamento_dbt/profiles.yml`

Crie ou substitua o conte√∫do do arquivo `treinamento_dbt/profiles.yml`:

```yaml
treinamento_dbt:
  target: "{{ env_var('DBT_TARGET', 'fabric_local') }}"
  outputs:
    # Perfil para Microsoft Fabric Local (Desenvolvimento com az login)
    fabric_local:
      type: fabric
      driver: "{{ env_var('ODBC_DRIVER', 'ODBC Driver 18 for SQL Server') }}"
      server: "{{ env_var('FABRIC_SERVER') }}"
      port: 1433
      database: "{{ env_var('FABRIC_DATABASE', 'DataWarehouseTreinamento') }}"
      schema: "{{ env_var('FABRIC_SCHEMA', 'dbo') }}"
      threads: "{{ env_var('DBT_THREADS', '4') | int }}"
      authentication: CLI
      encrypt: true
      trust_cert: false
    
    # Perfil para Microsoft Fabric (Service Principal - Produ√ß√£o/CI/CD)
    fabric:
      type: fabric
      driver: "{{ env_var('ODBC_DRIVER', 'ODBC Driver 18 for SQL Server') }}"
      server: "{{ env_var('FABRIC_SERVER') }}"
      port: 1433
      database: "{{ env_var('FABRIC_DATABASE', 'DataWarehouseTreinamento') }}"
      schema: "{{ env_var('FABRIC_SCHEMA', 'dbo') }}"
      threads: "{{ env_var('DBT_THREADS', '4') | int }}"
      authentication: ServicePrincipal
      tenant_id: "{{ env_var('FABRIC_TENANT_ID') }}"
      client_id: "{{ env_var('FABRIC_CLIENT_ID') }}"
      client_secret: "{{ env_var('FABRIC_CLIENT_SECRET') }}"
      encrypt: true
      trust_cert: false
```

### 5.2 Entendendo os perfis

**‚òÅÔ∏è fabric_local** - Desenvolvimento com Microsoft Fabric
- Usa autentica√ß√£o CLI (`az login`)
- N√£o precisa de Service Principal
- Usa suas credenciais pessoais do Azure

**üè≠ fabric** - Produ√ß√£o/CI/CD com Microsoft Fabric
- Usa autentica√ß√£o Service Principal
- Ideal para automa√ß√£o e pipelines
- Requer configura√ß√£o de App Registration no Azure

---

## üõ†Ô∏è 6. Scripts de Execu√ß√£o

### 6.1 Script PowerShell (Windows)

Crie o arquivo `run_dbt.ps1` na raiz do projeto:

```powershell
# Script para carregar vari√°veis de ambiente do .env e executar comandos DBT
# Uso: .\run_dbt.ps1 debug
#      .\run_dbt.ps1 run
#      .\run_dbt.ps1 test

param(
    [Parameter(Mandatory=$false)]
    [string]$Command = "debug"
)

# Carrega as vari√°veis do arquivo .env
$envFile = Join-Path $PSScriptRoot ".env"

if (Test-Path $envFile) {
    Write-Host "Carregando vari√°veis de ambiente de $envFile..." -ForegroundColor Green
    
    Get-Content $envFile | ForEach-Object {
        if ($_ -match '^\s*([^#][^=]+)=(.*)$') {
            $key = $matches[1].Trim()
            $value = $matches[2].Trim()
            
            # Remove aspas se existirem
            $value = $value -replace '^["'']|["'']$', ''
            
            # Define a vari√°vel de ambiente
            [Environment]::SetEnvironmentVariable($key, $value, "Process")
            Write-Host "  $key = $value" -ForegroundColor Cyan
        }
    }
    Write-Host ""
} else {
    Write-Host "Arquivo .env n√£o encontrado em $envFile" -ForegroundColor Yellow
}

# Ativa o ambiente virtual se existir
$venvPath = Join-Path $PSScriptRoot ".venv\Scripts\Activate.ps1"
if (Test-Path $venvPath) {
    Write-Host "Ativando ambiente virtual..." -ForegroundColor Green
    & $venvPath
    Write-Host ""
}

# Navega para o diret√≥rio do DBT
Set-Location (Join-Path $PSScriptRoot "treinamento_dbt")

# Executa o comando DBT
Write-Host "Executando: dbt $Command" -ForegroundColor Green
Write-Host ""

$dbtArgs = $Command -split ' '
& dbt @dbtArgs

# Retorna ao diret√≥rio original
Set-Location $PSScriptRoot
```

### 6.2 Script Bash (Linux/macOS)

Crie o arquivo `run_dbt.sh` na raiz do projeto:

```bash
#!/bin/bash
# Script para carregar vari√°veis de ambiente do .env e executar comandos DBT
# Uso: ./run_dbt.sh debug
#      ./run_dbt.sh run
#      ./run_dbt.sh test

# Carrega as vari√°veis do arquivo .env
if [ -f .env ]; then
    echo "Carregando vari√°veis de ambiente de .env..."
    export $(grep -v '^#' .env | xargs)
    echo ""
else
    echo "Arquivo .env n√£o encontrado"
    exit 1
fi

# Ativa o ambiente virtual se existir
if [ -f .venv/bin/activate ]; then
    echo "Ativando ambiente virtual..."
    source .venv/bin/activate
    echo ""
fi

# Navega para o diret√≥rio do DBT
cd treinamento_dbt

# Executa o comando DBT
echo "Executando: dbt $@"
echo ""
dbt "$@"

# Retorna ao diret√≥rio original
cd ..
```

**Tornar execut√°vel (Linux/macOS):**
```bash
chmod +x run_dbt.sh
```

---

## ‚úÖ 7. Valida√ß√£o da Configura√ß√£o

### 7.1 Testar conex√£o

**Windows:**
```powershell
.\run_dbt.ps1 debug
```

**Linux/macOS:**
```bash
./run_dbt.sh debug
```

### 7.2 Interpretar resultados

‚úÖ **Sucesso** - Todas as verifica√ß√µes passaram:
```
Configuration:
  profiles.yml file [OK found and valid]
  dbt_project.yml file [OK found and valid]
Connection test: [OK connection ok]
```

‚ùå **Erro** - Revise:
- Vari√°veis de ambiente no `.env`
- Driver ODBC instalado corretamente
- Credenciais v√°lidas
- Servidor acess√≠vel
- Para `fabric_local`: Execute `az login` primeiro

---

## üéØ 8. Execu√ß√£o do dbt

### 8.1 Comandos principais

**Validar conex√£o:**
```powershell
.\run_dbt.ps1 debug
```

**Executar todos os modelos:**
```powershell
.\run_dbt.ps1 run
```

**Executar modelo espec√≠fico:**
```powershell
.\run_dbt.ps1 "run --select my_model"
```

**Executar testes:**
```powershell
.\run_dbt.ps1 test
```

**Gerar documenta√ß√£o:**
```powershell
.\run_dbt.ps1 "docs generate"
.\run_dbt.ps1 "docs serve"
```

**Compilar sem executar:**
```powershell
.\run_dbt.ps1 compile
```

### 8.2 Alternar entre perfis

**Op√ß√£o 1 - Editar `.env`:**
```env
DBT_TARGET=fabric_local  # Para desenvolvimento com az login (padr√£o)
DBT_TARGET=fabric        # Para produ√ß√£o com Service Principal
```

**Op√ß√£o 2 - Vari√°vel tempor√°ria (PowerShell):**
```powershell
$env:DBT_TARGET="fabric_local"; .\run_dbt.ps1 debug
```

**Op√ß√£o 3 - Vari√°vel tempor√°ria (Bash):**
```bash
DBT_TARGET=fabric_local ./run_dbt.sh debug
```

---

## üîë 9. Configura√ß√£o de Autentica√ß√£o Fabric

### 9.1 Autentica√ß√£o CLI (fabric_local)

**Passo 1 - Login no Azure:**
```bash
az login
```

**Passo 2 - Verificar conta:**
```bash
az account show
```

**Passo 3 - Configurar `.env`:**
```env
DBT_TARGET=fabric_local
FABRIC_SERVER=seu-workspace.datawarehouse.fabric.microsoft.com
FABRIC_DATABASE=DataWarehouseTreinamento
FABRIC_SCHEMA=dbo
```

### 9.2 Autentica√ß√£o Service Principal (fabric)

**Passo 1 - Criar App Registration no Azure:**
1. Portal Azure ‚Üí Azure Active Directory
2. App registrations ‚Üí New registration
3. Copiar: **Application (client) ID**
4. Copiar: **Directory (tenant) ID**

**Passo 2 - Criar Client Secret:**
1. Certificates & secrets ‚Üí New client secret
2. Copiar o **Value** (aparece s√≥ uma vez!)

**Passo 3 - Dar permiss√µes no Fabric:**
1. Fabric workspace ‚Üí Settings ‚Üí Manage access
2. Adicionar o Service Principal como Admin/Member

**Passo 4 - Configurar `.env`:**
```env
DBT_TARGET=fabric
FABRIC_SERVER=seu-workspace.datawarehouse.fabric.microsoft.com
FABRIC_DATABASE=DataWarehouseTreinamento
FABRIC_SCHEMA=dbo
FABRIC_TENANT_ID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
FABRIC_CLIENT_ID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
FABRIC_CLIENT_SECRET=seu_secret_value
```

---

## ÔøΩ 10. Entendendo a Arquitetura do Projeto

Antes de come√ßarmos a construir os modelos, √© importante entender a arquitetura que seguiremos.

### 10.1 Arquitetura Medallion em 3 Camadas

Este projeto segue a arquitetura **Medallion**, organizando os dados em **3 camadas l√≥gicas**:

```text
ü•â BRONZE (Staging)      ‚Üí  ü•à SILVER (Intermediate)  ‚Üí  ü•á GOLD (Marts)
   Ingest√£o Bruta             Transforma√ß√µes                Consumo Final
   Views                      Views Reutiliz√°veis           Tables/Incremental
```

| Camada | Prop√≥sito | Materializa√ß√£o | Nomenclatura |
|--------|-----------|----------------|--------------|
| **ü•â Staging** | Ingest√£o 1:1 com fontes, limpeza b√°sica | `view` | `stg_{sistema}__{entidade}` |
| **ü•à Intermediate** | Transforma√ß√µes reutiliz√°veis, regras de neg√≥cio | `view` | `int_{conceito}` |
| **ü•á Marts** | Modelos finais para consumo (dimens√µes/fatos) | `table`/`incremental` | `dim_{entidade}` ou `fct_{processo}` |

### 10.2 Configura√ß√£o no dbt_project.yml

O arquivo `dbt_project.yml` j√° est√° configurado com estas defini√ß√µes:

```yaml
models:
  treinamento_dbt:
    staging:
      +materialized: view
      +schema: staging
    intermediate:
      +materialized: view
      +schema: intermediate
    marts:
      +materialized: table
      +schema: marts
```

### 10.3 Resultado no Microsoft Fabric

Quando executamos `dbt run`, o dbt criar√° os seguintes schemas no Fabric:

```text
DataWarehouseTreinamento/
‚îú‚îÄ‚îÄ dbo_staging/          # Views de ingest√£o
‚îú‚îÄ‚îÄ dbo_intermediate/     # Views de transforma√ß√£o
‚îî‚îÄ‚îÄ dbo_marts/            # Tabelas finais (dimensions + facts)
```

---

## üèóÔ∏è 11. CONSTRUINDO: Camada Staging (Bronze)

Agora vamos construir nossa primeira camada! A camada de **staging** faz a ingest√£o dos dados brutos.

### 11.1 Passo 1: Criar o diret√≥rio de staging

```bash
mkdir -p treinamento_dbt/models/staging/lakehouse
```

### 11.2 Passo 2: Documentar a Source

Crie o arquivo `treinamento_dbt/models/staging/lakehouse/_lakehouse_sources.yml`:

```yaml
version: 2

sources:
  - name: lakehouse_treinamento
    description: "Lakehouse contendo dados para treinamento do time"
    database: LakehouseTreinamento
    schema: dbo
    
    # Alertas se dados estiverem desatualizados
    freshness:
      warn_after: {count: 24, period: hour}
      error_after: {count: 7, period: day}
    
    tables:
      - name: taxi
        loaded_at_field: lpepPickupDatetime
        description: "Dados de viagens de t√°xi do lakehouse"
        columns:
          - name: vendorID
            description: "ID do fornecedor de dados (1=CMT, 2=VTS)"
            tests:
              - not_null
          
          - name: lpepPickupDatetime
            description: "Data e hora de in√≠cio da viagem"
            tests:
              - not_null
          
          - name: lpepDropoffDatetime
            description: "Data e hora de t√©rmino da viagem"
          
          - name: passengerCount
            description: "N√∫mero de passageiros na viagem"
          
          - name: tripDistance
            description: "Dist√¢ncia da viagem em milhas"
          
          - name: puLocationId
            description: "ID da localiza√ß√£o de pickup"
          
          - name: doLocationId
            description: "ID da localiza√ß√£o de dropoff"
          
          - name: paymentType
            description: "Tipo de pagamento (1=Cart√£o, 2=Dinheiro, 3=Sem cobran√ßa, etc.)"
          
          - name: fareAmount
            description: "Valor da tarifa"
          
          - name: totalAmount
            description: "Valor total da viagem"
```

### 11.3 Passo 3: Testar a Source

Verifique se o dbt consegue ler a source:

```powershell
.\run_dbt.ps1 "source freshness"
```

‚úÖ **Resultado esperado:** Confirma√ß√£o de que a tabela existe e est√° acess√≠vel.

### 11.4 Passo 4: Criar o Modelo Staging

Crie o arquivo `treinamento_dbt/models/staging/lakehouse/stg_lakehouse__taxi.sql`:

```sql
-- =====================================================
-- Staging: Dados de T√°xi do Lakehouse
-- =====================================================
-- Descri√ß√£o: Ingest√£o e limpeza b√°sica dos dados de viagens de t√°xi
-- Materializa√ß√£o: VIEW
-- Depend√™ncias: source('lakehouse_treinamento', 'taxi')
-- =====================================================

with source as (
    -- L√™ os dados da source documentada
    select * 
    from {{ source('lakehouse_treinamento', 'taxi') }}
),

filtered_source as (
    -- Filtra apenas dados at√© 2019
    select * 
    from source
    where year(lpepPickupDatetime) <= 2019 
       or year(lpepDropoffDatetime) <= 2019
),

unique_row as (
    -- Remove duplicatas mantendo maior totalAmount
    select *,
        ROW_NUMBER() OVER (
            PARTITION BY vendorID, lpepPickupDatetime 
            ORDER BY totalAmount DESC
        ) AS row_num
    from filtered_source
),

filtered as (
    -- Mant√©m apenas primeira linha de cada grupo
    select * 
    from unique_row
    where row_num = 1
),

with_sk_id as (
    -- Cria surrogate key √∫nica para cada viagem
    select *,
        HASHBYTES(
            'SHA2_256', 
            CONCAT_WS(
                '|',
                vendorID,
                CAST(lpepPickupDatetime AS VARCHAR(50))
            )
        ) AS sk_id
    from filtered
)

-- Retorna todos os campos incluindo a surrogate key
select * 
from with_sk_id
```

### 11.5 Passo 5: Executar o Modelo Staging

```powershell
.\run_dbt.ps1 "run --select stg_lakehouse__taxi"
```

‚úÖ **Resultado esperado:**
```
Completed successfully
1 of 1 OK created view model dbo_staging.stg_lakehouse__taxi
```

### 11.6 Passo 6: Explorar os Dados Criados

Agora que a view foi criada, vamos explorar os dados para entender o que foi constru√≠do.

**Consulta 1: Ver estrutura e primeiros registros**
```sql
-- Ver as primeiras 10 viagens
SELECT TOP 10 
    vendorID,
    lpepPickupDatetime,
    lpepDropoffDatetime,
    passengerCount,
    tripDistance,
    fareAmount,
    totalAmount,
    sk_id  -- Surrogate key criada pelo staging
FROM dbo_staging.stg_lakehouse__taxi
ORDER BY lpepPickupDatetime DESC;
```

**Consulta 2: Validar remo√ß√£o de duplicatas**
```sql
-- Verificar se existem duplicatas (deve retornar 0)
SELECT 
    vendorID,
    lpepPickupDatetime,
    COUNT(*) as qtd_registros
FROM dbo_staging.stg_lakehouse__taxi
GROUP BY vendorID, lpepPickupDatetime
HAVING COUNT(*) > 1;
```

**Consulta 3: Estat√≠sticas gerais**
```sql
-- Estat√≠sticas dos dados de staging
SELECT 
    COUNT(*) as total_viagens,
    COUNT(DISTINCT vendorID) as total_vendors,
    MIN(lpepPickupDatetime) as primeira_viagem,
    MAX(lpepPickupDatetime) as ultima_viagem,
    AVG(tripDistance) as distancia_media,
    AVG(totalAmount) as valor_medio,
    SUM(totalAmount) as receita_total
FROM dbo_staging.stg_lakehouse__taxi;
```

**Consulta 4: Distribui√ß√£o por vendor**
```sql
-- Quantas viagens por fornecedor
SELECT 
    vendorID,
    COUNT(*) as total_viagens,
    AVG(tripDistance) as distancia_media,
    AVG(totalAmount) as valor_medio,
    MIN(lpepPickupDatetime) as primeira_viagem,
    MAX(lpepPickupDatetime) as ultima_viagem
FROM dbo_staging.stg_lakehouse__taxi
GROUP BY vendorID
ORDER BY vendorID;
```

**Consulta 5: Validar filtro de ano**
```sql
-- Verificar se todas as viagens s√£o at√© 2019
SELECT 
    YEAR(lpepPickupDatetime) as ano_pickup,
    YEAR(lpepDropoffDatetime) as ano_dropoff,
    COUNT(*) as quantidade
FROM dbo_staging.stg_lakehouse__taxi
GROUP BY YEAR(lpepPickupDatetime), YEAR(lpepDropoffDatetime)
ORDER BY ano_pickup, ano_dropoff;
```

üí° **Interpreta√ß√£o dos Resultados:**
- **Total de viagens**: Deve mostrar todas as viagens ap√≥s remo√ß√£o de duplicatas
- **Surrogate key (sk_id)**: Cada viagem tem um identificador √∫nico em hash
- **Anos**: Apenas viagens at√© 2019 devem aparecer
- **Vendors**: Normalmente 2 vendors (1=CMT, 2=VTS)

### 11.7 Passo 7: Validar Qualidade dos Dados

Execute consultas para garantir a qualidade:

```sql
-- 1. Verificar registros com valores nulos em campos cr√≠ticos
SELECT 
    COUNT(*) as total,
    COUNT(vendorID) as com_vendor,
    COUNT(lpepPickupDatetime) as com_pickup_date,
    COUNT(totalAmount) as com_total_amount,
    COUNT(*) - COUNT(vendorID) as sem_vendor,
    COUNT(*) - COUNT(lpepPickupDatetime) as sem_pickup_date
FROM dbo_staging.stg_lakehouse__taxi;

-- 2. Verificar viagens com valores negativos (n√£o deveria haver)
SELECT 
    COUNT(*) as viagens_valor_negativo
FROM dbo_staging.stg_lakehouse__taxi
WHERE totalAmount < 0 OR fareAmount < 0 OR tripDistance < 0;

-- 3. Verificar viagens com data de dropoff antes de pickup (anomalia)
SELECT 
    COUNT(*) as viagens_anomalas,
    MIN(DATEDIFF(MINUTE, lpepPickupDatetime, lpepDropoffDatetime)) as menor_duracao_minutos
FROM dbo_staging.stg_lakehouse__taxi
WHERE lpepDropoffDatetime < lpepPickupDatetime;
```

‚úÖ **Resultados esperados:**
- Campos cr√≠ticos n√£o devem ter nulos (vendorID, lpepPickupDatetime)
- N√£o deve haver valores negativos em dist√¢ncia/valores
- Dropoff sempre deve ser ap√≥s pickup

### 11.8 Passo 8: Validar no Fabric (Interface Gr√°fica)

Acesse o **Microsoft Fabric** e verifique:
1. Schema `dbo_staging` foi criado
2. View `stg_lakehouse__taxi` existe
3. Consulte alguns registros para validar

```sql
SELECT TOP 10 * 
FROM dbo_staging.stg_lakehouse__taxi
```

‚úÖ **Checkpoint:** Camada Staging criada com sucesso! Voc√™ agora tem uma view limpa dos dados brutos com qualidade validada.

---

## üîÑ 12. CONSTRUINDO: Camada Intermediate (Silver)

A camada intermediate prepara componentes reutiliz√°veis. Aqui criaremos **6 dimens√µes intermedi√°rias** a partir dos dados de t√°xi.

### 12.1 Criar o diret√≥rio intermediate

```bash
mkdir -p treinamento_dbt/models/intermediate/lakehouse
```

### 12.2 Modelo 1: int_dim_date (Calend√°rio Completo)

**Objetivo:** Gerar todas as datas entre a menor e maior data das viagens com atributos de calend√°rio.

Crie `treinamento_dbt/models/intermediate/lakehouse/int_dim_date.sql`:

```sql
-- =====================================================
-- Intermediate: Dimens√£o de Data
-- =====================================================
-- Gera um calend√°rio completo entre min e max das viagens
-- com todos os atributos de data √∫teis para an√°lise
--=====================================================

WITH source_data AS (
    SELECT *
    from {{ ref('stg_lakehouse__taxi') }}
    WHERE year(lpepPickupDatetime) <= 2019 OR year(lpepDropoffDatetime) <= 2019
),

date_bounds AS (
    -- Encontra a data m√≠nima e m√°xima
    SELECT 
        MIN(CAST(lpepPickupDatetime AS DATE)) AS min_date,
        MAX(CAST(lpepPickupDatetime AS DATE)) AS max_date
    FROM source_data
),

-- Gerador de n√∫meros (0 a 9)
ten_rows AS (
    SELECT 1 AS n UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL
    SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1
),

-- Gera sequ√™ncia grande de dias (10 x 10 x 10 x 10 x 10 = 100.000 dias ~ 274 anos)
number_series AS (
    SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS n
    FROM ten_rows a
    CROSS JOIN ten_rows b
    CROSS JOIN ten_rows c
    CROSS JOIN ten_rows d
    CROSS JOIN ten_rows e
),

-- Gera as datas brutas
raw_dates AS (
    SELECT 
        CAST(DATEADD(DAY, ns.n, db.min_date) AS DATE) AS date_value
    FROM date_bounds db
    CROSS JOIN number_series ns
    WHERE DATEADD(DAY, ns.n, db.min_date) <= db.max_date
),

-- Adiciona todos os atributos de data
date_attributes AS (
    SELECT
        date_value AS date,
        YEAR(date_value) AS year,
        MONTH(date_value) AS month,
        DAY(date_value) AS day_of_month,
        DATENAME(MONTH, date_value) AS month_name,
        LEFT(DATENAME(MONTH, date_value), 3) AS month_name_abbrev,
        DATENAME(WEEKDAY, date_value) AS day_of_week_name,
        LEFT(DATENAME(WEEKDAY, date_value), 3) AS day_of_week_abbrev,
        DATEPART(WEEKDAY, date_value) AS day_of_week_num,
        CASE WHEN DATEPART(WEEKDAY, date_value) IN (1, 7) THEN 1 ELSE 0 END AS is_weekend,
        DATEPART(QUARTER, date_value) AS quarter,
        CONCAT('Q', DATEPART(QUARTER, date_value), '-', YEAR(date_value)) AS year_quarter,
        CEILING(MONTH(date_value) / 2.0) AS bimester,
        CEILING(MONTH(date_value) / 6.0) AS semester,
        CONCAT('S', CEILING(MONTH(date_value) / 6.0), '-', YEAR(date_value)) AS year_semester,
        CEILING(DAY(date_value) / 15.0) AS fortnight,
        DATEPART(DAYOFYEAR, date_value) AS day_of_year,
        DATEPART(WEEK, date_value) AS week_of_year
    FROM raw_dates
)

SELECT * FROM date_attributes
ORDER BY date
```

### 12.3 Modelo 2: int_dim_location (Localiza√ß√µes)

**Objetivo:** Extrair todos os IDs √∫nicos de localiza√ß√£o (pickup + dropoff).

Crie `treinamento_dbt/models/intermediate/lakehouse/int_dim_location.sql`:

```sql
-- =====================================================
-- Intermediate: Dimens√£o de Localiza√ß√£o
-- =====================================================
-- Extrai IDs √∫nicos de localiza√ß√£o (pickup e dropoff)
-- e cria surrogate key para cada localiza√ß√£o
-- =====================================================

with sg_taxi as (
    select *
    from {{ ref('stg_lakehouse__taxi') }}
),

location as (
    -- Uni√£o de localiza√ß√µes de pickup
    select distinct puLocationId as location_id
    from sg_taxi
    
    UNION
    
    -- Uni√£o de localiza√ß√µes de dropoff
    select distinct doLocationId as location_id
    from sg_taxi
),

dim_location as (
    select
        distinct location_id,
        HASHBYTES(
            'SHA2_256',
            CAST(location_id AS VARCHAR(50))
        ) AS sk_location_id
    from location
)

select *
from dim_location
```

### 12.4 Modelo 3: int_dim_vendor (Fornecedores)

**Objetivo:** Criar dimens√£o de fornecedores com nomes descritivos.

Crie `treinamento_dbt/models/intermediate/lakehouse/int_dim_vendor.sql`:

```sql
-- =====================================================
-- Intermediate: Dimens√£o de Fornecedor
-- =====================================================
-- Lista de fornecedores de dados com nomes descritivos
-- =====================================================

WITH vendor_data AS (
    -- Busca os IDs √∫nicos da camada de staging
    SELECT DISTINCT 
        CAST("vendorID" AS INT) AS vendor_id
    from {{ ref('stg_lakehouse__taxi') }}
    WHERE "vendorID" IS NOT NULL
)

SELECT
    vendor_id,
    CASE vendor_id
        WHEN 1 THEN 'Creative Mobile Technologies'
        WHEN 2 THEN 'VeriFone Inc.'
        ELSE 'Unknown/Other'
    END AS vendor_name,
    CASE vendor_id
        WHEN 1 THEN 'CMT'
        WHEN 2 THEN 'VTS'
        ELSE 'UNK'
    END AS vendor_abbreviation
FROM vendor_data
```

### 12.5 Modelo 4: int_dim_payment_type (Tipos de Pagamento)

Crie `treinamento_dbt/models/intermediate/lakehouse/int_dim_payment_type.sql`:

```sql
-- =====================================================
-- Intermediate: Dimens√£o de Tipo de Pagamento
-- =====================================================
-- Mapeia c√≥digos de pagamento para descri√ß√µes leg√≠veis
-- =====================================================

WITH data AS (
    SELECT DISTINCT paymentType as payment_type
    FROM {{ ref('stg_lakehouse__taxi') }}
    WHERE paymentType IS NOT NULL
)

SELECT
    payment_type,
    CASE payment_type
        WHEN 1 THEN 'Credit card'
        WHEN 2 THEN 'Cash'
        WHEN 3 THEN 'No charge'
        WHEN 4 THEN 'Dispute'
        WHEN 5 THEN 'Unknown'
        ELSE 'Not Specified'
    END AS payment_type_name
FROM data
```

### 12.6 Modelo 5: int_dim_rate_code (C√≥digos de Tarifa)

Crie `treinamento_dbt/models/intermediate/lakehouse/int_dim_rate_code.sql`:

```sql
-- =====================================================
-- Intermediate: Dimens√£o de Rate Code
-- =====================================================
-- Mapeia c√≥digos de tarifa para descri√ß√µes leg√≠veis
-- =====================================================

WITH data AS (
    SELECT DISTINCT rateCodeId as rate_code_id
    FROM {{ ref('stg_lakehouse__taxi') }}
    WHERE rateCodeId IS NOT NULL
)

SELECT
    rate_code_id,
    CASE rate_code_id
        WHEN 1 THEN 'Standard rate'
        WHEN 2 THEN 'JFK'
        WHEN 3 THEN 'Newark'
        WHEN 4 THEN 'Nassau or Westchester'
        WHEN 5 THEN 'Negotiated fare'
        WHEN 6 THEN 'Group ride'
        ELSE 'Unknown'
    END AS rate_code_name
FROM data
```

### 12.7 Modelo 6: int_dim_time (Hor√°rios do Dia)

Crie `treinamento_dbt/models/intermediate/lakehouse/int_dim_time.sql`:

```sql
-- =====================================================
-- Intermediate: Dimens√£o de Tempo (Hor√°rio do Dia)
-- =====================================================
-- Gera todos os hor√°rios do dia (00:00 a 23:59)
-- com atributos √∫teis para an√°lise temporal
-- =====================================================

WITH hour_series AS (
    -- Gera n√∫meros de 0 a 23 (horas)
    SELECT TOP 24 ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS hour_num
    FROM sys.objects
),

minute_series AS (
    -- Gera n√∫meros de 0 a 59 (minutos)
    SELECT TOP 60 ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS minute_num
    FROM sys.objects
),

time_combinations AS (
    SELECT
        h.hour_num,
        m.minute_num,
        CAST(FORMAT(h.hour_num, '00') + ':' + FORMAT(m.minute_num, '00') + ':00' AS TIME) AS time_value
    FROM hour_series h
    CROSS JOIN minute_series m
),

time_attributes AS (
    SELECT
        time_value,
        hour_num AS hour,
        minute_num AS minute,
        CASE 
            WHEN hour_num BETWEEN 0 AND 5 THEN 'Madrugada'
            WHEN hour_num BETWEEN 6 AND 11 THEN 'Manh√£'
            WHEN hour_num BETWEEN 12 AND 17 THEN 'Tarde'
            ELSE 'Noite'
        END AS period_of_day,
        CASE 
            WHEN hour_num BETWEEN 7 AND 9 OR hour_num BETWEEN 17 AND 19 THEN 1
            ELSE 0
        END AS is_rush_hour
    FROM time_combinations
)

SELECT * FROM time_attributes
ORDER BY time_value
```

### 12.8 Executar Todos os Modelos Intermediate

```powershell
.\run_dbt.ps1 "run --select intermediate"
```

‚úÖ **Resultado esperado:**
```
Completed successfully
6 of 6 OK created view model dbo_intermediate.int_dim_date
6 of 6 OK created view model dbo_intermediate.int_dim_location  
6 of 6 OK created view model dbo_intermediate.int_dim_vendor
6 of 6 OK created view model dbo_intermediate.int_dim_payment_type
6 of 6 OK created view model dbo_intermediate.int_dim_rate_code
6 of 6 OK created view model dbo_intermediate.int_dim_time
```

### 12.9 Explorar os Dados Criados

Agora vamos validar cada dimens√£o intermedi√°ria criada com consultas pr√°ticas.

**üìÖ Consulta 1: Dimens√£o de Data (int_dim_date)**
```sql
-- Ver primeiras e √∫ltimas datas do calend√°rio
SELECT TOP 5 
    date,
    year,
    month,
    month_name,
    day_of_week_name,
    is_weekend,
    quarter,
    year_quarter
FROM dbo_intermediate.int_dim_date
ORDER BY date;

-- √öltima data
SELECT TOP 5 
    date,
    day_of_week_name,
    is_weekend
FROM dbo_intermediate.int_dim_date
ORDER BY date DESC;

-- Estat√≠sticas do calend√°rio
SELECT 
    COUNT(*) as total_dias,
    MIN(date) as primeira_data,
    MAX(date) as ultima_data,
    COUNT(CASE WHEN is_weekend = 1 THEN 1 END) as dias_fim_semana,
    COUNT(CASE WHEN is_weekend = 0 THEN 1 END) as dias_uteis
FROM dbo_intermediate.int_dim_date;
```

**üìç Consulta 2: Dimens√£o de Localiza√ß√£o (int_dim_location)**
```sql
-- Ver todas as localiza√ß√µes √∫nicas
SELECT TOP 10
    location_id,
    sk_location_id
FROM dbo_intermediate.int_dim_location
ORDER BY location_id;

-- Estat√≠sticas
SELECT 
    COUNT(*) as total_localizacoes,
    MIN(location_id) as menor_id,
    MAX(location_id) as maior_id
FROM dbo_intermediate.int_dim_location;
```

**üöñ Consulta 3: Dimens√£o de Vendor (int_dim_vendor)**
```sql
-- Ver todos os vendors
SELECT 
    vendor_id,
    vendor_name,
    vendor_abbreviation
FROM dbo_intermediate.int_dim_vendor
ORDER BY vendor_id;
```

**üí≥ Consulta 4: Dimens√£o de Tipo de Pagamento (int_dim_payment_type)**
```sql
-- Ver todos os tipos de pagamento
SELECT 
    payment_type,
    payment_type_name
FROM dbo_intermediate.int_dim_payment_type
ORDER BY payment_type;
```

**üí∞ Consulta 5: Dimens√£o de Rate Code (int_dim_rate_code)**
```sql
-- Ver todos os c√≥digos de tarifa
SELECT 
    rate_code_id,
    rate_code_name
FROM dbo_intermediate.int_dim_rate_code
ORDER BY rate_code_id;
```

**üïê Consulta 6: Dimens√£o de Tempo (int_dim_time)**
```sql
-- Ver primeiros hor√°rios do dia
SELECT TOP 20
    time_value,
    hour,
    minute,
    period_of_day,
    is_rush_hour
FROM dbo_intermediate.int_dim_time
ORDER BY time_value;

-- Hor√°rios de pico (rush hour)
SELECT 
    time_value,
    hour,
    period_of_day
FROM dbo_intermediate.int_dim_time
WHERE is_rush_hour = 1
ORDER BY time_value;

-- Estat√≠sticas por per√≠odo do dia
SELECT 
    period_of_day,
    COUNT(*) as total_minutos,
    COUNT(CASE WHEN is_rush_hour = 1 THEN 1 END) as minutos_pico
FROM dbo_intermediate.int_dim_time
GROUP BY period_of_day
ORDER BY 
    CASE period_of_day
        WHEN 'Madrugada' THEN 1
        WHEN 'Manh√£' THEN 2
        WHEN 'Tarde' THEN 3
        WHEN 'Noite' THEN 4
    END;
```

**üìä Consulta 7: Resumo Geral de Todas as Dimens√µes**
```sql
-- Contagem de registros em cada dimens√£o intermediate
SELECT 'int_dim_date' as dimensao, COUNT(*) as total_registros 
FROM dbo_intermediate.int_dim_date
UNION ALL
SELECT 'int_dim_location', COUNT(*) 
FROM dbo_intermediate.int_dim_location
UNION ALL
SELECT 'int_dim_vendor', COUNT(*) 
FROM dbo_intermediate.int_dim_vendor
UNION ALL
SELECT 'int_dim_payment_type', COUNT(*) 
FROM dbo_intermediate.int_dim_payment_type
UNION ALL
SELECT 'int_dim_rate_code', COUNT(*) 
FROM dbo_intermediate.int_dim_rate_code
UNION ALL
SELECT 'int_dim_time', COUNT(*) 
FROM dbo_intermediate.int_dim_time
ORDER BY total_registros DESC;
```

üí° **Resultados Esperados:**
- **int_dim_date**: ~1.500 datas (do m√≠nimo ao m√°ximo das viagens)
- **int_dim_location**: ~260 localiza√ß√µes √∫nicas
- **int_dim_vendor**: 2 vendors (CMT e VTS)
- **int_dim_payment_type**: ~5 tipos de pagamento
- **int_dim_rate_code**: ~6 c√≥digos de tarifa
- **int_dim_time**: 1.440 registros (24h √ó 60min)

### 12.10 Validar no Fabric
```sql
-- Verificar quantidade de registros em cada view
SELECT COUNT(*) FROM dbo_intermediate.int_dim_date;        -- ~1.500 datas
SELECT COUNT(*) FROM dbo_intermediate.int_dim_location;    -- ~260 localiza√ß√µes
SELECT COUNT(*) FROM dbo_intermediate.int_dim_vendor;      -- 2 vendors
SELECT COUNT(*) FROM dbo_intermediate.int_dim_payment_type; -- ~5 tipos
SELECT COUNT(*) FROM dbo_intermediate.int_dim_rate_code;   -- ~6 c√≥digos
SELECT COUNT(*) FROM dbo_intermediate.int_dim_time;        -- 1.440 minutos (24h x 60min)
```

‚úÖ **Checkpoint:** Camada Intermediate criada! Agora temos componentes reutiliz√°veis prontos e validados com dados reais.

---

## üéØ 13. CONSTRUINDO: Camada Marts - Dimensions (Gold)

Agora vamos criar as **dimens√µes finais** do Data Warehouse. Estas ser√£o **tabelas materializadas** para melhor performance.

### 13.1 Criar o diret√≥rio marts

```bash
mkdir -p treinamento_dbt/models/marts/dimensions
```

### 13.2 Dimens√£o 1: dim_date (Incremental)

**Objetivo:** Dimens√£o de data otimizada com carga incremental.

Crie `treinamento_dbt/models/marts/dimensions/dim_date.sql`:

```sql
-- =====================================================
-- Marts: Dimens√£o de Data (INCREMENTAL)
-- =====================================================
-- Dimens√£o conformed de data para todo o Data Warehouse
-- Materializa√ß√£o: INCREMENTAL para performance
-- =====================================================

{{
    config(
        materialized='incremental',
        unique_key='date',
        on_schema_change='fail'
    )
}}

WITH data AS (
    SELECT *
    FROM {{ ref ('int_dim_date') }}
    {% if is_incremental() %}
    -- Processa apenas datas que ainda n√£o existem na tabela
    WHERE date > (SELECT MAX(date) FROM {{ this }})
    {% endif %}
)

SELECT 
    date,
    year,
    month,
    day_of_month,
    month_name,
    month_name_abbrev,
    day_of_week_name,
    day_of_week_abbrev,
    day_of_week_num,
    is_weekend,
    quarter,
    year_quarter,
    bimester,
    semester,
    year_semester,
    fortnight,
    day_of_year,
    week_of_year
FROM data
ORDER BY date
```

Crie `treinamento_dbt/models/marts/dimensions/dim_date.yml`:

```yaml
version: 2

models:
  - name: dim_date
    description: "Dimens√£o conformed de data com todos os atributos de calend√°rio"
    columns:
      - name: date
        description: "Data (chave prim√°ria)"
        tests:
          - unique
          - not_null
      
      - name: year
        description: "Ano (YYYY)"
        tests:
          - not_null
      
      - name: month
        description: "M√™s (1-12)"
        tests:
          - not_null
      
      - name: is_weekend
        description: "Indicador de final de semana (0=N√£o, 1=Sim)"
      
      - name: quarter
        description: "Trimestre (1-4)"
```

### 13.3 Dimens√£o 2: dim_location

Crie `treinamento_dbt/models/marts/dimensions/dim_location.sql`:

```sql
-- =====================================================
-- Marts: Dimens√£o de Localiza√ß√£o
-- =====================================================

WITH data AS (
    SELECT *
    FROM {{ ref ('int_dim_location') }}
)

SELECT 
    location_id,
    sk_location_id
FROM data
ORDER BY location_id
```

Crie `treinamento_dbt/models/marts/dimensions/dim_location.yml`:

```yaml
version: 2

models:
  - name: dim_location
    description: "Dimens√£o de localiza√ß√µes (zonas de t√°xi)"
    columns:
      - name: location_id
        description: "ID da localiza√ß√£o (chave prim√°ria)"
        tests:
          - unique
          - not_null
      
      - name: sk_location_id
        description: "Surrogate key da localiza√ß√£o"
```

### 13.4 Dimens√£o 3: dim_vendor

Crie `treinamento_dbt/models/marts/dimensions/dim_vendor.sql`:

```sql
-- =====================================================
-- Marts: Dimens√£o de Fornecedor
-- =====================================================

WITH data AS (
    SELECT *
    FROM {{ ref ('int_dim_vendor') }}
)

SELECT 
    vendor_id,
    vendor_name,
    vendor_abbreviation
FROM data
ORDER BY vendor_id
```

Crie `treinamento_dbt/models/marts/dimensions/dim_vendor.yml`:

```yaml
version: 2

models:
  - name: dim_vendor
    description: "Dimens√£o de fornecedores de dados"
    columns:
      - name: vendor_id
        description: "ID do fornecedor (chave prim√°ria)"
        tests:
          - unique
          - not_null
      
      - name: vendor_name
        description: "Nome completo do fornecedor"
      
      - name: vendor_abbreviation
        description: "Sigla do fornecedor"
```

### 13.5 Dimens√£o 4: dim_payment_type

Crie `treinamento_dbt/models/marts/dimensions/dim_payment_type.sql`:

```sql
-- =====================================================
-- Marts: Dimens√£o de Tipo de Pagamento
-- =====================================================

WITH data AS (
    SELECT *
    FROM {{ ref ('int_dim_payment_type') }}
)

SELECT 
    payment_type,
    payment_type_name
FROM data
ORDER BY payment_type
```

Crie `treinamento_dbt/models/marts/dimensions/dim_payment_type.yml`:

```yaml
version: 2

models:
  - name: dim_payment_type
    description: "Dimens√£o de tipos de pagamento"
    columns:
      - name: payment_type
        description: "C√≥digo do tipo de pagamento (chave prim√°ria)"
        tests:
          - unique
          - not_null
      
      - name: payment_type_name
        description: "Descri√ß√£o do tipo de pagamento"
```

### 13.6 Dimens√£o 5: dim_rate_code

Crie `treinamento_dbt/models/marts/dimensions/dim_rate_code.sql`:

```sql
-- =====================================================
-- Marts: Dimens√£o de Rate Code
-- =====================================================

WITH data AS (
    SELECT *
    FROM {{ ref ('int_dim_rate_code') }}
)

SELECT 
    rate_code_id,
    rate_code_name
FROM data
ORDER BY rate_code_id
```

Crie `treinamento_dbt/models/marts/dimensions/dim_rate_code.yml`:

```yaml
version: 2

models:
  - name: dim_rate_code
    description: "Dimens√£o de c√≥digos de tarifa"
    columns:
      - name: rate_code_id
        description: "ID do c√≥digo de tarifa (chave prim√°ria)"
        tests:
          - unique
          - not_null
      
      - name: rate_code_name
        description: "Descri√ß√£o do c√≥digo de tarifa"
```

### 13.7 Dimens√£o 6: dim_time

Crie `treinamento_dbt/models/marts/dimensions/dim_time.sql`:

```sql
-- =====================================================
-- Marts: Dimens√£o de Tempo
-- =====================================================

WITH data AS (
    SELECT *
    FROM {{ ref ('int_dim_time') }}
)

SELECT 
    time_value,
    hour,
    minute,
    period_of_day,
    is_rush_hour
FROM data
ORDER BY time_value
```

Crie `treinamento_dbt/models/marts/dimensions/dim_time.yml`:

```yaml
version: 2

models:
  - name: dim_time
    description: "Dimens√£o de tempo (hor√°rios do dia)"
    columns:
      - name: time_value
        description: "Hora e minuto (HH:MM:SS) - chave prim√°ria"
        tests:
          - unique
          - not_null
      
      - name: hour
        description: "Hora (0-23)"
      
      - name: minute
        description: "Minuto (0-59)"
      
      - name: period_of_day
        description: "Per√≠odo do dia (Madrugada, Manh√£, Tarde, Noite)"
      
      - name: is_rush_hour
        description: "Indica hor√°rio de pico (0=N√£o, 1=Sim)"
```

### 13.8 Executar Todas as Dimens√µes

```powershell
.\run_dbt.ps1 "run --select marts.dimensions"
```

‚úÖ **Resultado esperado:**
```
Completed successfully
6 of 6 OK created table model dbo_marts.dim_date
6 of 6 OK created table model dbo_marts.dim_location
6 of 6 OK created table model dbo_marts.dim_vendor
6 of 6 OK created table model dbo_marts.dim_payment_type
6 of 6 OK created table model dbo_marts.dim_rate_code
6 of 6 OK created table model dbo_marts.dim_time
```

### 13.9 Executar Testes de Qualidade

```powershell
.\run_dbt.ps1 "test --select marts.dimensions"
```

‚úÖ **Resultado esperado:** Todos os testes de `unique` e `not_null` devem passar.

### 13.10 Explorar as Dimens√µes Criadas

Agora que as dimens√µes finais foram materializadas como **tabelas**, vamos explorar os dados com consultas anal√≠ticas.

**üìÖ Consulta 1: Explorar dim_date (Dimens√£o de Data)**
```sql
-- Ver estrutura completa da dimens√£o
SELECT TOP 10 
    date,
    year,
    month,
    day_of_month,
    month_name,
    day_of_week_name,
    is_weekend,
    quarter,
    year_quarter,
    semester,
    week_of_year
FROM dbo_marts.dim_date
ORDER BY date DESC;

-- An√°lise de finais de semana por ano
SELECT 
    year,
    COUNT(*) as total_dias,
    SUM(is_weekend) as dias_fim_semana,
    COUNT(*) - SUM(is_weekend) as dias_uteis,
    CAST(SUM(is_weekend) * 100.0 / COUNT(*) AS DECIMAL(5,2)) as pct_fim_semana
FROM dbo_marts.dim_date
GROUP BY year
ORDER BY year;

-- Distribui√ß√£o de dias por trimestre
SELECT 
    year_quarter,
    COUNT(*) as total_dias,
    MIN(date) as primeira_data,
    MAX(date) as ultima_data
FROM dbo_marts.dim_date
GROUP BY year_quarter
ORDER BY year_quarter;
```

**üìç Consulta 2: Explorar dim_location (Dimens√£o de Localiza√ß√£o)**
```sql
-- Ver todas as localiza√ß√µes
SELECT 
    location_id,
    sk_location_id
FROM dbo_marts.dim_location
ORDER BY location_id;

-- Estat√≠sticas de localiza√ß√µes
SELECT 
    COUNT(*) as total_localizacoes,
    MIN(location_id) as menor_location_id,
    MAX(location_id) as maior_location_id
FROM dbo_marts.dim_location;
```

**üöñ Consulta 3: Explorar dim_vendor (Dimens√£o de Fornecedor)**
```sql
-- Ver detalhes de todos os vendors
SELECT 
    vendor_id,
    vendor_name,
    vendor_abbreviation
FROM dbo_marts.dim_vendor
ORDER BY vendor_id;
```

**üí≥ Consulta 4: Explorar dim_payment_type (Dimens√£o de Pagamento)**
```sql
-- Ver todos os tipos de pagamento dispon√≠veis
SELECT 
    payment_type,
    payment_type_name
FROM dbo_marts.dim_payment_type
ORDER BY payment_type;
```

**üí∞ Consulta 5: Explorar dim_rate_code (Dimens√£o de Tarifa)**
```sql
-- Ver todos os c√≥digos de tarifa
SELECT 
    rate_code_id,
    rate_code_name
FROM dbo_marts.dim_rate_code
ORDER BY rate_code_id;
```

**üïê Consulta 6: Explorar dim_time (Dimens√£o de Tempo)**
```sql
-- Ver primeiros e √∫ltimos hor√°rios
SELECT TOP 10
    time_value,
    hour,
    minute,
    period_of_day,
    is_rush_hour
FROM dbo_marts.dim_time
ORDER BY time_value;

-- An√°lise de hor√°rios de pico por per√≠odo do dia
SELECT 
    period_of_day,
    COUNT(*) as total_minutos,
    SUM(is_rush_hour) as minutos_pico,
    CAST(SUM(is_rush_hour) * 100.0 / COUNT(*) AS DECIMAL(5,2)) as pct_pico
FROM dbo_marts.dim_time
GROUP BY period_of_day
ORDER BY 
    CASE period_of_day
        WHEN 'Madrugada' THEN 1
        WHEN 'Manh√£' THEN 2
        WHEN 'Tarde' THEN 3
        WHEN 'Noite' THEN 4
    END;
```

**üîç Consulta 7: An√°lise Cross-Dimensional (Combinando Dimens√µes)**
```sql
-- Criar dataset combinado para an√°lise
-- Exemplo: Todas as combina√ß√µes de Data x Vendor x Tipo de Pagamento
SELECT TOP 100
    d.date,
    d.day_of_week_name,
    d.is_weekend,
    v.vendor_name,
    p.payment_type_name,
    r.rate_code_name
FROM dbo_marts.dim_date d
CROSS JOIN dbo_marts.dim_vendor v
CROSS JOIN dbo_marts.dim_payment_type p
CROSS JOIN dbo_marts.dim_rate_code r
WHERE d.year = 2019 AND d.month = 12  -- Filtro para reduzir resultados
ORDER BY d.date DESC, v.vendor_name, p.payment_type_name;

-- An√°lise de dias √∫teis vs finais de semana
SELECT 
    CASE WHEN d.is_weekend = 1 THEN 'Fim de Semana' ELSE 'Dia √ötil' END as tipo_dia,
    COUNT(DISTINCT d.date) as total_dias
FROM dbo_marts.dim_date d
GROUP BY d.is_weekend
ORDER BY d.is_weekend;
```

**üìä Consulta 8: Resumo Geral de Todas as Dimens√µes**
```sql
-- Contagem de registros em cada dimens√£o final
SELECT 'dim_date' as dimensao, COUNT(*) as total_registros, 'Tabela' as tipo
FROM dbo_marts.dim_date
UNION ALL
SELECT 'dim_location', COUNT(*), 'Tabela'
FROM dbo_marts.dim_location
UNION ALL
SELECT 'dim_vendor', COUNT(*), 'Tabela'
FROM dbo_marts.dim_vendor
UNION ALL
SELECT 'dim_payment_type', COUNT(*), 'Tabela'
FROM dbo_marts.dim_payment_type
UNION ALL
SELECT 'dim_rate_code', COUNT(*), 'Tabela'
FROM dbo_marts.dim_rate_code
UNION ALL
SELECT 'dim_time', COUNT(*), 'Tabela'
FROM dbo_marts.dim_time
ORDER BY total_registros DESC;
```

üí° **Insights das Dimens√µes:**
- **dim_date**: Base temporal completa para an√°lises hist√≥ricas
- **dim_location**: Permite an√°lise geogr√°fica das viagens
- **dim_vendor**: Compara√ß√£o entre fornecedores de dados
- **dim_payment_type**: An√°lise de prefer√™ncias de pagamento
- **dim_rate_code**: An√°lise de tipos de tarifas aplicadas
- **dim_time**: An√°lise de padr√µes hor√°rios e per√≠odos do dia

### 13.11 Validar no Fabric

```sql
-- Validar dimens√µes criadas no schema marts
SELECT COUNT(*) FROM dbo_marts.dim_date;          -- ~1.500 registros
SELECT COUNT(*) FROM dbo_marts.dim_location;      -- ~260 registros
SELECT COUNT(*) FROM dbo_marts.dim_vendor;        -- 2 registros
SELECT COUNT(*) FROM dbo_marts.dim_payment_type;  -- ~5 registros
SELECT COUNT(*) FROM dbo_marts.dim_rate_code;     -- ~6 registros
SELECT COUNT(*) FROM dbo_marts.dim_time;          -- 1.440 registros

-- Testar consulta anal√≠tica
SELECT TOP 10 
    d.date,
    d.day_of_week_name,
    d.is_weekend,
    v.vendor_name
FROM dbo_marts.dim_date d
CROSS JOIN dbo_marts.dim_vendor v
ORDER BY d.date DESC;
```

‚úÖ **Checkpoint:** Data Warehouse dimensional criado! Todas as 6 dimens√µes est√£o prontas, validadas e com dados explor√°veis.

---

## üìñ 14. Documenta√ß√£o e Lineage

### 14.1 Gerar Documenta√ß√£o

O dbt gera documenta√ß√£o autom√°tica com lineage (linhagem) dos dados:

```powershell
.\run_dbt.ps1 "docs generate"
```

### 14.2 Visualizar Documenta√ß√£o

```powershell
.\run_dbt.ps1 "docs serve"
```

Isso abrir√° um servidor local (geralmente `http://localhost:8080`) com:
- üìä **Lineage Graph** - Visualiza√ß√£o do fluxo de dados
- üìù **Documenta√ß√£o** - Descriptions de todos os modelos
- ‚úÖ **Testes** - Status de todos os testes
- üìà **M√©tricas** - Tempo de execu√ß√£o, rows processadas

### 14.3 Navegar na Documenta√ß√£o

1. **Project** ‚Üí Navegue pelos modelos
2. **Database** ‚Üí Veja os objetos criados no Fabric
3. **Graph** ‚Üí Explore o lineage visual:
   - Verde = Sources
   - Azul = Models
   - Linhas = Depend√™ncias

### 14.4 Compartilhar Documenta√ß√£o

Para compartilhar a documenta√ß√£o com o time:

**Op√ß√£o 1 - Hospedar em GitHub Pages:**
1. Gerar docs: `dbt docs generate`
2. Copiar `/target/index.html` e `/target/catalog.json`
3. Publicar em GitHub Pages

**Op√ß√£o 2 - Usar dbt Cloud:**
- Upload autom√°tico da documenta√ß√£o
- Acesso via web para todo o time

---

## üõ†Ô∏è 15. Comandos √öteis do Dia a Dia

### 15.1 Execu√ß√£o Seletiva

```powershell
# Executar apenas staging
.\run_dbt.ps1 "run --select staging"

# Executar apenas intermediate
.\run_dbt.ps1 "run --select intermediate"

# Executar apenas marts
.\run_dbt.ps1 "run --select marts"

# Executar um modelo espec√≠fico
.\run_dbt.ps1 "run --select dim_date"

# Executar um modelo e suas depend√™ncias
.\run_dbt.ps1 "run --select +dim_date"

# Executar um modelo e seus dependentes
.\run_dbt.ps1 "run --select dim_date+"

# Executar modelos modificados
.\run_dbt.ps1 "run --select state:modified+"
```

### 15.2 Testes

```powershell
# Executar todos os testes
.\run_dbt.ps1 test

# Testar apenas staging
.\run_dbt.ps1 "test --select staging"

# Testar um modelo espec√≠fico
.\run_dbt.ps1 "test --select dim_date"
```

### 15.3 Compila√ß√£o e Debug

```powershell
# Compilar sem executar (ver SQL gerado)
.\run_dbt.ps1 compile

# Ver SQL compilado de um modelo
.\run_dbt.ps1 "compile --select dim_date"
# Resultado em: treinamento_dbt/target/compiled/

# Debug de conex√£o
.\run_dbt.ps1 debug
```

### 15.4 Freshness de Sources

```powershell
# Verificar atualiza√ß√£o das sources
.\run_dbt.ps1 "source freshness"
```

### 15.5 Limpeza

```powershell
# Limpar diret√≥rio target
.\run_dbt.ps1 clean
```

---

## üîç 16. Troubleshooting

### 16.1 Erros de Conex√£o

**Erro: "environment variable 'XXX' not found"**
```
Solu√ß√£o:
1. Verifique se o arquivo .env existe na raiz do projeto
2. Confirme que est√° usando os scripts run_dbt.ps1 ou run_dbt.sh
3. Verifique se as vari√°veis est√£o definidas corretamente no .env
```

**Erro: "Unable to connect to database"**
```
Solu√ß√£o:
1. Verifique se FABRIC_SERVER est√° correto no.env
2. Para fabric_local: Execute 'az login' e verifique a conta ativa com 'az account show'
3. Para fabric: Valide tenant_id, client_id e client_secret
4. Teste conectividade com Azure Data Studio ou outra ferramenta SQL
5. Confirme que voc√™ tem permiss√µes no workspace Fabric
```

**Erro: "Authentication failed"**
```
Solu√ß√£o:
1. Para fabric_local: Token pode ter expirado, execute 'az login' novamente
2. Para fabric: Verifique as credenciais do Service Principal
3. Confirme que o Service Principal tem permiss√µes Admin/Member no workspace
```

### 16.2 Erros de Execu√ß√£o

**Erro: "Compilation Error" ou "Syntax Error"**
```
Solu√ß√£o:
1. Execute 'dbt compile' para ver o SQL gerado
2. Verifique se h√° erros de sintaxe SQL no modelo
3. Confirme que todas as refer√™ncias {{ ref() }} e {{ source() }} est√£o corretas
4. Verifique se os nomes de colunas correspondem aos dados reais
```

**Erro: "Relation does not exist"**
```
Solu√ß√£o:
1. Verifique se a source ou modelo referenciado existe
2. Execute os modelos upstream primeiro (ex: staging antes de intermediate)
3. Confirme que o schema est√° correto no profiles.yml
```

**Erro: "Column not found"**
```
Solu√ß√£o:
1. Verifique se a coluna existe na tabela fonte
2. Confirme se o nome da coluna est√° correto (case-sensitive)
3. Execute SELECT * na source para ver estrutura real
```

### 16.3 Erros de Performance

**Modelo muito lento**
```
Solu√ß√£o:
1. Considere usar materializa√ß√£o 'table' ao inv√©s de 'view'
2. Para tabelas grandes, use materializa√ß√£o 'incremental'
3. Adicione √≠ndices nas tabelas finais (ap√≥s o dbt run)
4. Revise queries para otimizar joins e filtros
```

**Erro: "Timeout" ou "Memory exceeded"**
```
Solu√ß√£o:
1. Aumente o valor de DBT_THREADS no .env (ex: DBT_THREADS=2)
2. Execute modelos em lotes menores com --select
3. Use materializa√ß√£o incremental para tabelas grandes
```

### 16.4 Erros de Testes

**Teste failing: "unique" ou "not_null"**
```
Solu√ß√£o:
1. Execute query direto no Fabric para investigar:
   SELECT column_name, COUNT(*) 
   FROM schema.table 
   GROUP BY column_name 
   HAVING COUNT(*) > 1
2. Adicione filtros ou transforma√ß√µes no modelo para corrigir dados
3. Se esperado, remova ou ajuste o teste
```

### 16.5 Dicas de Debug

**Ver SQL compilado:**
```powershell
.\run_dbt.ps1 compile
# Resultado em: treinamento_dbt/target/compiled/...
```

**Executar apenas um modelo para testar:**
```powershell
.\run_dbt.ps1 "run --select dim_date --full-refresh"
```

**Ver logs detalhados:**
```powershell
.\run_dbt.ps1 "run --select dim_date --debug"
```

**Verificar depend√™ncias de um modelo:**
```powershell
.\run_dbt.ps1 "list --select +dim_date+"
```

---

## üìö 17. Recursos e Pr√≥ximos Passos

### 17.1 Documenta√ß√£o Oficial

- [dbt Documentation](https://docs.getdbt.com/) - Documenta√ß√£o completa do dbt
- [dbt-fabric Adapter](https://docs.getdbt.com/docs/core/connect-data-platform/fabric-setup) - Espec√≠fico para Microsoft Fabric
- [Microsoft Fabric Docs](https://learn.microsoft.com/en-us/fabric/) - Documenta√ß√£o do Fabric
- [dbt Best Practices](https://docs.getdbt.com/guides/best-practices) - Boas pr√°ticas oficiais

### 17.2 Evolu√ß√£o do Projeto

**Pr√≥ximos passos recomendados:**

1. **‚úÖ Criar Tabelas Fato**
   - `fct_taxi_trips` - Fato de viagens com m√©tricas
   - Joins com todas as dimens√µes criadas
   - Materializa√ß√£o incremental

2. **üìä Implementar M√©tricas**
   - Criar m√©tricas reutiliz√°veis com dbt metrics
   - Total de viagens, receita m√©dia, dist√¢ncia total, etc.

3. **üì∏ Adicionar Snapshots (SCD Type 2)**
   - Rastrear mudan√ßas hist√≥ricas em dimens√µes
   - Manter hist√≥rico de altera√ß√µes

4. **üîç Testes Customizados**
   - Criar macros de testes espec√≠ficos do neg√≥cio
   - Valida√ß√µes de regras de neg√≥cio complexas

5. **ü§ñ CI/CD com GitHub Actions**
   - Automatizar execu√ß√£o do dbt em PRs
   - Deploy autom√°tico em produ√ß√£o
   - Testes autom√°ticos antes de merge

6. **üìà Monitoring e Alertas**
   - Configurar alertas de freshness
   - Monitorar tempo de execu√ß√£o
   - Dashboard de quality checks

7. **üîê Governan√ßa de Dados**
   - Adicionar tags para classifica√ß√£o
   - Documentar ownership de modelos
   - Implementar access control

### 17.3 Boas Pr√°ticas

**Sempre fa√ßa:**
- ‚úÖ Versionar c√≥digo no Git com commits descritivos
- ‚úÖ Nunca commitar credenciais (usar `.env`)
- ‚úÖ Documentar todos os modelos com `description`
- ‚úÖ Implementar testes de qualidade em campos cr√≠ticos
- ‚úÖ Usar nomenclatura consistente (prefixos: stg_, int_, dim_, fct_)
- ‚úÖ Revisar c√≥digo via Pull Requests
- ‚úÖ Executar `dbt test` antes de fazer merge
- ‚úÖ Manter `README.md` atualizado

**Evite:**
- ‚ùå Hardcodear valores (usar vari√°veis e macros)
- ‚ùå Criar depend√™ncias circulares entre modelos
- ‚ùå Modelos muito complexos (quebrar em intermediate)
- ‚ùå Commit direto em main/master
- ‚ùå Pular testes por "falta de tempo"

### 17.4 Estrutura de Branches Recomendada

```text
main/master     ‚Üí Produ√ß√£o (fabric com Service Principal)
develop         ‚Üí Desenvolvimento (fabric_local com az login)
feature/*       ‚Üí Features individuais
hotfix/*        ‚Üí Corre√ß√µes urgentes
```

### 17.5 Template de Commit

```bash
# Formato sugerido
<tipo>: <descri√ß√£o curta>

<descri√ß√£o detalhada opcional>

Tipos:
- feat: Nova feature
- fix: Corre√ß√£o de bug
- docs: Documenta√ß√£o
- refactor: Refatora√ß√£o
- test: Adicionar testes
- chore: Manuten√ß√£o

Exemplo:
feat: adicionar dimens√£o de tempo com per√≠odo do dia

- Criar int_dim_time com gera√ß√£o de hor√°rios
- Adicionar dim_time no marts
- Incluir atributos de per√≠odo e hor√°rio de pico
```

### 17.6 Comunidade e Suporte

- [dbt Community Slack](https://www.getdbt.com/community/join-the-community/) - Comunidade ativa
- [dbt Discourse](https://discourse.getdbt.com/) - F√≥rum de discuss√µes
- [GitHub dbt-core](https://github.com/dbt-labs/dbt-core) - Issues e contribui√ß√µes
- [GitHub dbt-fabric](https://github.com/microsoft/dbt-fabric) - Adapter espec√≠fico

---

## üéì 18. Observa√ß√µes Finais

### ‚ú® O que voc√™ construiu:

Este projeto implementa um **Data Warehouse dimensional completo** seguindo as melhores pr√°ticas de engenharia de dados:

**üìä Arquitetura:**
- ü•â **Camada Bronze (Staging)**: 1 source, 1 modelo de ingest√£o
- ü•à **Camada Silver (Intermediate)**: 6 dimens√µes intermedi√°rias
- ü•á **Camada Gold (Marts)**: 6 dimens√µes finais otimizadas

**üîß Infraestrutura:**
- ‚òÅÔ∏è Totalmente integrado com **Microsoft Fabric**
- üîê Seguran√ßa via vari√°veis de ambiente e `.gitignore`
- üöÄ Scripts automatizados para facilitar execu√ß√£o
- üìù Documenta√ß√£o completa com lineage autom√°tico
- ‚úÖ Testes de qualidade implementados

**üìà Boas Pr√°ticas:**
- Arquitetura Medallion (Bronze/Silver/Gold)
- Separa√ß√£o de responsabilidades por camada
- Nomenclatura consistente e descritiva
- C√≥digo versionado e documentado
- Estrat√©gias de materializa√ß√£o otimizadas

### üéØ Benef√≠cios Alcan√ßados:

1. **Replicabilidade** - Qualquer pessoa pode seguir este README e recriar o projeto
2. **Manutenibilidade** - C√≥digo organizado e documentado
3. **Escalabilidade** - Arquitetura preparada para crescer
4. **Qualidade** - Testes garantem integridade dos dados
5. **Performance** - Materializa√ß√µes otimizadas por caso de uso
6. **Colabora√ß√£o** - Documenta√ß√£o facilita trabalho em equipe

### üöÄ Pr√≥xima Jornada:

Voc√™ agora tem uma **base s√≥lida** para:
- Adicionar novas fontes de dados
- Criar tabelas fato complexas
- Implementar m√©tricas de neg√≥cio
- Automatizar com CI/CD
- Escalar para produ√ß√£o

---

**üéâ Parab√©ns!** Voc√™ completou o treinamento de dbt com Microsoft Fabric.

Este projeto est√° pronto para ser usado como:
- üìö **Material de treinamento** para novos membros do time
- üèóÔ∏è **Template** para novos projetos dbt
- üìñ **Refer√™ncia** de boas pr√°ticas
- üéØ **Base** para evolu√ß√£o cont√≠nua

**Continue aprendendo e construindo! üöÄ**

---

_√öltima atualiza√ß√£o: Fevereiro 2026_
